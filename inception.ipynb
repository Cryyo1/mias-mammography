{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import wget\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from training_utils import download_file, get_batches, read_and_decode_single_example, load_validation_data, \\\n",
    "    download_data, evaluate_model, get_training_data, load_weights, flatten, _conv2d_batch_norm\n",
    "from inception_utils import _stem, _block_a, _block_b, _block_c, _reduce_a, _reduce_b\n",
    "import argparse\n",
    "from tensorboard import summary as summary_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "dataset = 9\n",
    "init_model = None\n",
    "how = \"normal\"\n",
    "action = \"train\"\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 1366\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# precalculated pixel mean of images\n",
    "mu = 104.1353\n",
    "\n",
    "# download the data\n",
    "download_data(what=dataset)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_files, total_records = get_training_data(what=dataset)\n",
    "\n",
    "## Hyperparameters\n",
    "# Small epsilon value for the BN transform\n",
    "epsilon = 1e-8\n",
    "\n",
    "# learning rate\n",
    "epochs_per_decay = 10\n",
    "starting_rate = 0.001\n",
    "decay_factor = 0.80\n",
    "staircase = True\n",
    "\n",
    "# learning rate decay variables\n",
    "steps_per_epoch = int(total_records / batch_size)\n",
    "print(\"Steps per epoch:\", steps_per_epoch)\n",
    "\n",
    "# lambdas\n",
    "lamC = 0.00000\n",
    "lamF = 0.00250\n",
    "\n",
    "# use dropout\n",
    "dropout = False\n",
    "fcdropout_rate = 0.5\n",
    "convdropout_rate = 0.0\n",
    "pooldropout_rate = 0.0\n",
    "\n",
    "if how == \"label\":\n",
    "    num_classes = 5\n",
    "elif how == \"normal\":\n",
    "    num_classes = 2\n",
    "elif how == \"mass\":\n",
    "    num_classes = 3\n",
    "elif how == \"benign\":\n",
    "    num_classes = 3\n",
    "\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "Predictions: (?,)\n",
      "Graph created...\n"
     ]
    }
   ],
   "source": [
    "## Build the graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "# whether to retrain model from scratch or use saved model\n",
    "init = True\n",
    "model_name = \"vgg_16.3.01l.6\"\n",
    "# vgg_19.01 - attempting to recreate vgg 19 architecture\n",
    "# vgg_16.02 - went to vgg 16 architecture, reducing units in fc layers\n",
    "# vgg_16.2.01 - changing first conv layers to stride 2 to get dimensions down to reasonable size\n",
    "# vgg_16.2.02 - using normal x-entropy instead of weighted\n",
    "# vgg_16.3.01 - average pooling image before first conv, changing conv1 to stride 1\n",
    "\n",
    "with graph.as_default():\n",
    "    training = tf.placeholder(dtype=tf.bool, name=\"is_training\")\n",
    "    is_testing = tf.placeholder(dtype=bool, shape=(), name=\"is_testing\")\n",
    "\n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(starting_rate,\n",
    "                                               global_step,\n",
    "                                               steps_per_epoch * epochs_per_decay,\n",
    "                                               decay_factor,\n",
    "                                               staircase=staircase)\n",
    "\n",
    "    with tf.name_scope('inputs') as scope:\n",
    "        image, label = read_and_decode_single_example(train_files, label_type=how, normalize=False)\n",
    "\n",
    "        X_def, y_def = tf.train.shuffle_batch([image, label], batch_size=batch_size, capacity=2000,\n",
    "                                              min_after_dequeue=1000)\n",
    "\n",
    "        # Placeholders\n",
    "        X = tf.placeholder_with_default(X_def, shape=[None, 299, 299, 1])\n",
    "        y = tf.placeholder_with_default(y_def, shape=[None])\n",
    "\n",
    "        X = tf.cast(X, dtype=tf.float32)\n",
    "\n",
    "        # center the pixel data\n",
    "        mu = tf.constant(mu, name=\"pixel_mean\", dtype=tf.float32)\n",
    "        X = tf.subtract(X, mu, name=\"centered_input\")\n",
    "\n",
    "    # input stem\n",
    "    stem = _stem(X, lamC, training)\n",
    "\n",
    "    # 4 Block As\n",
    "    blocka = _block_a(stem, name=\"a_1.1\", lamC=0.0, training = training)\n",
    "    blocka = _block_a(blocka, name=\"a_1.2\", lamC=0.0, training=training)\n",
    "    blocka = _block_a(blocka, name=\"a_1.3\", lamC=0.0, training=training)\n",
    "    blocka = _block_a(blocka, name=\"a_1.4\", lamC=0.0, training=training)\n",
    "\n",
    "    # Reduction A\n",
    "    reducea = _reduce_a(blocka, \"a_reduce_1\", k=192, l=224, m=256, n=384, training = training)\n",
    "\n",
    "    # 7 Block Bs\n",
    "    blockb = _block_b(reducea, \"b_1.1\", lamC=0.0, training = training)\n",
    "    blockb = _block_b(blockb, \"b_1.2\", lamC=0.0, training=training)\n",
    "    blockb = _block_b(blockb, \"b_1.3\", lamC=0.0, training=training)\n",
    "    blockb = _block_b(blockb, \"b_1.4\", lamC=0.0, training=training)\n",
    "    blockb = _block_b(blockb, \"b_1.5\", lamC=0.0, training=training)\n",
    "    blockb = _block_b(blockb, \"b_1.6\", lamC=0.0, training=training)\n",
    "    blockb = _block_b(blockb, \"b_1.7\", lamC=0.0, training=training)\n",
    "\n",
    "    # Reduction B\n",
    "    reduceb = _reduce_b(blockb, name=\"b_reduce_1\", training = training)\n",
    "\n",
    "    # 3 Block Cs\n",
    "    blockc = _block_c(reduceb, name=\"c_1.1\", lamC=0.0, training = training)\n",
    "    blockc = _block_c(blockc, name=\"c_1.2\", lamC=0.0, training=training)\n",
    "    blockc = _block_c(blockc, name=\"c_1.3\", lamC=0.0, training=training)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    global_pool = tf.layers.average_pooling2d(\n",
    "            blockc,  # Input\n",
    "            pool_size=(8, 8),  # Pool size: 2x2\n",
    "            strides=(8, 8),  # Stride: 2\n",
    "            padding='SAME',  # \"same\" padding\n",
    "            name='global_pool'\n",
    "        )\n",
    "\n",
    "    global_pool = tf.layers.dropout(global_pool, rate=0.2, seed=103, training=training)\n",
    "    \n",
    "    # flatten the output\n",
    "    flat_output = tf.contrib.layers.flatten(global_pool)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        flat_output,\n",
    "        num_classes,  # One output unit per category\n",
    "        activation=None,  # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=121),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name=\"fc_logits\"\n",
    "    )\n",
    "\n",
    "    # get the fully connected variables so we can only train them when retraining the network\n",
    "    fc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"fc\")\n",
    "\n",
    "    # with tf.variable_scope('conv_1.1', reuse=True):\n",
    "    #     conv_kernels1 = tf.get_variable('kernel')\n",
    "    #     kernel_transposed = tf.transpose(conv_kernels1, [3, 0, 1, 2])\n",
    "    #\n",
    "    # with tf.variable_scope('visualization'):\n",
    "    #     tf.summary.image('conv_1.1/filters', kernel_transposed, max_outputs=32, collections=[\"kernels\"])\n",
    "\n",
    "    # get the probabilites for the classes\n",
    "    probabilities = tf.nn.softmax(logits, name=\"probabilities\")\n",
    "\n",
    "    # the probability that the scan is abnormal is 1 - probability it is normal\n",
    "    abnormal_probability = (1 - probabilities[:, 0])\n",
    "\n",
    "    if num_classes > 2:\n",
    "        # the scan is abnormal if the probability is greater than the threshold\n",
    "        #is_abnormal = tf.cast(tf.greater(abnormal_probability, threshold), tf.int64)\n",
    "\n",
    "        # Compute predictions from the probabilities - if the scan is normal we ignore the other probabilities\n",
    "        #predictions = is_abnormal * tf.argmax(probabilities[:,1:], axis=1, output_type=tf.int64)\n",
    "        predictions = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
    "    else:\n",
    "        predictions = tf.cast(tf.greater(abnormal_probability, threshold), tf.int32)\n",
    "    \n",
    "    print(\"Predictions:\", predictions.shape)\n",
    "    \n",
    "    # get the accuracy\n",
    "    accuracy, acc_op = tf.metrics.accuracy(\n",
    "        labels=y,\n",
    "        predictions=predictions,\n",
    "        updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
    "        # metrics_collections=[\"summaries\"],\n",
    "        name=\"accuracy\",\n",
    "    )\n",
    "\n",
    "    #########################################################\n",
    "    ## Loss function options\n",
    "    # Regular mean cross entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "    #########################################################\n",
    "    ## Weight the positive examples higher\n",
    "    # This will weight the positive examples higher so as to improve recall\n",
    "    #weights = tf.multiply(1, tf.cast(tf.greater(y, 0), tf.int32)) + 1\n",
    "    #mean_ce = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits, weights=weights))\n",
    "\n",
    "    # Add in l2 loss\n",
    "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    # Minimize cross-entropy\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # calculate recall\n",
    "    if num_classes > 2:\n",
    "        # collapse the predictions down to normal or not for our pr metrics\n",
    "        zero = tf.constant(0, dtype=tf.int64)\n",
    "        collapsed_predictions = tf.cast(tf.greater(predictions, zero), tf.int64)\n",
    "        collapsed_labels = tf.greater(y, zero)\n",
    "\n",
    "        recall, rec_op = tf.metrics.recall(labels=collapsed_labels, predictions=collapsed_predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"recall\")\n",
    "        precision, prec_op = tf.metrics.precision(labels=collapsed_labels, predictions=collapsed_predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"precision\")\n",
    "\n",
    "    else:\n",
    "        recall, rec_op = tf.metrics.recall(labels=y, predictions=predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"recall\")\n",
    "        precision, prec_op = tf.metrics.precision(labels=y, predictions=predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"precision\")\n",
    "\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "    _, update_op = summary_lib.pr_curve_streaming_op(name='pr_curve',\n",
    "                                                     predictions=(1 - probabilities[:, 0]),\n",
    "                                                     labels=y,\n",
    "                                                     updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
    "                                                     num_thresholds=20)\n",
    "\n",
    "    tf.summary.scalar('recall_1', recall, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('precision_1', precision, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('f1_score', f1_score, collections=[\"summaries\"])\n",
    "\n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('cross_entropy', mean_ce, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('learning_rate', learning_rate, collections=[\"summaries\"])\n",
    "\n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    # Merge all the summaries\n",
    "    merged = tf.summary.merge_all(\"summaries\")\n",
    "    kernel_summaries = tf.summary.merge_all(\"kernels\")\n",
    "    per_epoch_summaries = [[]]\n",
    "\n",
    "    print(\"Graph created...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CONFIGURE OPTIONS\n",
    "if init_model is not None:\n",
    "    if os.path.exists(os.path.join(\"model\", init_model + '.ckpt.index')):\n",
    "        init = False\n",
    "    else:\n",
    "        init = True\n",
    "else:\n",
    "    if os.path.exists(os.path.join(\"model\", model_name + '.ckpt.index')):\n",
    "        init = False\n",
    "    else:\n",
    "        init = True\n",
    "\n",
    "meta_data_every = 1\n",
    "log_to_tensorboard = True\n",
    "print_every = 5  # how often to print metrics\n",
    "checkpoint_every = 1  # how often to save model in epochs\n",
    "use_gpu = False  # whether or not to use the GPU\n",
    "print_metrics = True  # whether to print or plot metrics, if False a plot will be created and updated every epoch\n",
    "\n",
    "# Initialize metrics or load them from disk if they exist\n",
    "if os.path.exists(os.path.join(\"data\", model_name + \"train_acc.npy\")):\n",
    "    train_acc_values = np.load(os.path.join(\"data\", model_name + \"train_acc.npy\")).tolist()\n",
    "else:\n",
    "    train_acc_values = []\n",
    "\n",
    "if os.path.exists(os.path.join(\"data\", model_name + \"train_loss.npy\")):\n",
    "    train_cost_values = np.load(os.path.join(\"data\", model_name + \"train_loss.npy\")).tolist()\n",
    "else:\n",
    "    train_cost_values = []\n",
    "\n",
    "if os.path.exists(os.path.join(\"data\", model_name + \"train_lr.npy\")):\n",
    "    train_lr_values = np.load(os.path.join(\"data\", model_name + \"train_lr.npy\")).tolist()\n",
    "else:\n",
    "    train_lr_values = []\n",
    "\n",
    "if os.path.exists(os.path.join(\"data\", model_name + \"train_recall.npy\")):\n",
    "    train_recall_values = np.load(os.path.join(\"data\", model_name + \"train_recall.npy\")).tolist()\n",
    "else:\n",
    "    train_recall_values = []\n",
    "\n",
    "if os.path.exists(os.path.join(\"data\", model_name + \"cv_acc.npy\")):\n",
    "    valid_acc_values = np.load(os.path.join(\"data\", model_name + \"cv_acc.npy\")).tolist()\n",
    "else:\n",
    "    valid_acc_values = []\n",
    "\n",
    "if os.path.exists(os.path.join(\"data\", model_name + \"cv_loss.npy\")):\n",
    "    valid_cost_values = np.load(os.path.join(\"data\", model_name + \"cv_loss.npy\")).tolist()\n",
    "else:\n",
    "    valid_cost_values = []\n",
    "\n",
    "if os.path.exists(os.path.join(\"data\", model_name + \"cv_recall.npy\")):\n",
    "    valid_recall_values = np.load(os.path.join(\"data\", model_name + \"cv_recall.npy\")).tolist()\n",
    "else:\n",
    "    valid_recall_values = []\n",
    "\n",
    "config = tf.ConfigProto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Training model vgg_16.3.01l.6 ...\n",
      "INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: inputs/input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](inputs/input_producer, inputs/input_producer/RandomShuffle)]]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[32,64,147,147] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: layer_stem_1.3/conv_stem_1.3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](layer_stem_1.2/relu_stem_1.2, conv_stem_1.3/kernel/read)]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  86.72MiB from layer_stem_1.2/bn_stem_1.2/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/bn_stem_1.1/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/conv_stem_1.1/Conv2D\n  86.72MiB from gradients/AddN_470-0-TransposeNHWCToNCHW-LayoutOptimizer\n  84.41MiB from layer_stem_1.2/conv_stem_1.2/Conv2D\n  84.41MiB from gradients/zeros_884-0-1-TransposeNCHWToNHWC-LayoutOptimizer\n  10.91MiB from inputs/Cast\n  Remaining 3 nodes with 768B\n\n\t [[Node: layer_c_1.3c_branch_4_conv_1.4/bn_c_1.3c_branch_4_conv_1.4/AssignMovingAvg_1/_1281 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_29840...ovingAvg_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  86.72MiB from layer_stem_1.2/bn_stem_1.2/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/bn_stem_1.1/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/conv_stem_1.1/Conv2D\n  86.72MiB from gradients/AddN_470-0-TransposeNHWCToNCHW-LayoutOptimizer\n  84.41MiB from layer_stem_1.2/conv_stem_1.2/Conv2D\n  84.41MiB from gradients/zeros_884-0-1-TransposeNCHWToNHWC-LayoutOptimizer\n  10.91MiB from inputs/Cast\n  Remaining 3 nodes with 768B\n\n\nCaused by op 'layer_stem_1.3/conv_stem_1.3/Conv2D', defined at:\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-3ebcd3c9e312>\", line 43, in <module>\n    stem = _stem(X, lamC, training)\n  File \"C:\\Users\\eric\\Documents\\Courses\\Applied ML 2\\mammography\\inception_utils.py\", line 13, in _stem\n    padding=\"SAME\", lambd=lamC, name=\"stem_1.3\")\n  File \"C:\\Users\\eric\\Documents\\Courses\\Applied ML 2\\mammography\\training_utils.py\", line 490, in _conv2d_batch_norm\n    name='conv_'+name\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 619, in conv2d\n    return layer.apply(inputs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 825, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 714, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 168, in call\n    outputs = self._convolution_op(inputs, self.kernel)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 870, in __call__\n    return self.conv_op(inp, filter)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 522, in __call__\n    return self.call(inp, filter)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 206, in __call__\n    name=self.name)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1039, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[32,64,147,147] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: layer_stem_1.3/conv_stem_1.3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](layer_stem_1.2/relu_stem_1.2, conv_stem_1.3/kernel/read)]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  86.72MiB from layer_stem_1.2/bn_stem_1.2/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/bn_stem_1.1/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/conv_stem_1.1/Conv2D\n  86.72MiB from gradients/AddN_470-0-TransposeNHWCToNCHW-LayoutOptimizer\n  84.41MiB from layer_stem_1.2/conv_stem_1.2/Conv2D\n  84.41MiB from gradients/zeros_884-0-1-TransposeNCHWToNHWC-LayoutOptimizer\n  10.91MiB from inputs/Cast\n  Remaining 3 nodes with 768B\n\n\t [[Node: layer_c_1.3c_branch_4_conv_1.4/bn_c_1.3c_branch_4_conv_1.4/AssignMovingAvg_1/_1281 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_29840...ovingAvg_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  86.72MiB from layer_stem_1.2/bn_stem_1.2/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/bn_stem_1.1/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/conv_stem_1.1/Conv2D\n  86.72MiB from gradients/AddN_470-0-TransposeNHWCToNCHW-LayoutOptimizer\n  84.41MiB from layer_stem_1.2/conv_stem_1.2/Conv2D\n  84.41MiB from gradients/zeros_884-0-1-TransposeNCHWToNHWC-LayoutOptimizer\n  10.91MiB from inputs/Cast\n  Remaining 3 nodes with 768B\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    515\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    517\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,64,147,147] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: layer_stem_1.3/conv_stem_1.3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](layer_stem_1.2/relu_stem_1.2, conv_stem_1.3/kernel/read)]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  86.72MiB from layer_stem_1.2/bn_stem_1.2/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/bn_stem_1.1/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/conv_stem_1.1/Conv2D\n  86.72MiB from gradients/AddN_470-0-TransposeNHWCToNCHW-LayoutOptimizer\n  84.41MiB from layer_stem_1.2/conv_stem_1.2/Conv2D\n  84.41MiB from gradients/zeros_884-0-1-TransposeNCHWToNHWC-LayoutOptimizer\n  10.91MiB from inputs/Cast\n  Remaining 3 nodes with 768B\n\n\t [[Node: layer_c_1.3c_branch_4_conv_1.4/bn_c_1.3c_branch_4_conv_1.4/AssignMovingAvg_1/_1281 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_29840...ovingAvg_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  86.72MiB from layer_stem_1.2/bn_stem_1.2/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/bn_stem_1.1/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/conv_stem_1.1/Conv2D\n  86.72MiB from gradients/AddN_470-0-TransposeNHWCToNCHW-LayoutOptimizer\n  84.41MiB from layer_stem_1.2/conv_stem_1.2/Conv2D\n  84.41MiB from gradients/zeros_884-0-1-TransposeNCHWToNHWC-LayoutOptimizer\n  10.91MiB from inputs/Cast\n  Remaining 3 nodes with 768B\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c6eaac4142fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m                                 },\n\u001b[0;32m     84\u001b[0m                                 \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                                 run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;31m# every 50th step get the metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1338\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1339\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1340\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,64,147,147] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: layer_stem_1.3/conv_stem_1.3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](layer_stem_1.2/relu_stem_1.2, conv_stem_1.3/kernel/read)]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  86.72MiB from layer_stem_1.2/bn_stem_1.2/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/bn_stem_1.1/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/conv_stem_1.1/Conv2D\n  86.72MiB from gradients/AddN_470-0-TransposeNHWCToNCHW-LayoutOptimizer\n  84.41MiB from layer_stem_1.2/conv_stem_1.2/Conv2D\n  84.41MiB from gradients/zeros_884-0-1-TransposeNCHWToNHWC-LayoutOptimizer\n  10.91MiB from inputs/Cast\n  Remaining 3 nodes with 768B\n\n\t [[Node: layer_c_1.3c_branch_4_conv_1.4/bn_c_1.3c_branch_4_conv_1.4/AssignMovingAvg_1/_1281 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_29840...ovingAvg_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  86.72MiB from layer_stem_1.2/bn_stem_1.2/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/bn_stem_1.1/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/conv_stem_1.1/Conv2D\n  86.72MiB from gradients/AddN_470-0-TransposeNHWCToNCHW-LayoutOptimizer\n  84.41MiB from layer_stem_1.2/conv_stem_1.2/Conv2D\n  84.41MiB from gradients/zeros_884-0-1-TransposeNCHWToNHWC-LayoutOptimizer\n  10.91MiB from inputs/Cast\n  Remaining 3 nodes with 768B\n\n\nCaused by op 'layer_stem_1.3/conv_stem_1.3/Conv2D', defined at:\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 478, in start\n    self.io_loop.start()\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-3ebcd3c9e312>\", line 43, in <module>\n    stem = _stem(X, lamC, training)\n  File \"C:\\Users\\eric\\Documents\\Courses\\Applied ML 2\\mammography\\inception_utils.py\", line 13, in _stem\n    padding=\"SAME\", lambd=lamC, name=\"stem_1.3\")\n  File \"C:\\Users\\eric\\Documents\\Courses\\Applied ML 2\\mammography\\training_utils.py\", line 490, in _conv2d_batch_norm\n    name='conv_'+name\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 619, in conv2d\n    return layer.apply(inputs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 825, in apply\n    return self.__call__(inputs, *args, **kwargs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\", line 714, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\", line 168, in call\n    outputs = self._convolution_op(inputs, self.kernel)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 870, in __call__\n    return self.conv_op(inp, filter)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 522, in __call__\n    return self.call(inp, filter)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 206, in __call__\n    name=self.name)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 1039, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3290, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1654, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[32,64,147,147] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: layer_stem_1.3/conv_stem_1.3/Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](layer_stem_1.2/relu_stem_1.2, conv_stem_1.3/kernel/read)]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  86.72MiB from layer_stem_1.2/bn_stem_1.2/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/bn_stem_1.1/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/conv_stem_1.1/Conv2D\n  86.72MiB from gradients/AddN_470-0-TransposeNHWCToNCHW-LayoutOptimizer\n  84.41MiB from layer_stem_1.2/conv_stem_1.2/Conv2D\n  84.41MiB from gradients/zeros_884-0-1-TransposeNCHWToNHWC-LayoutOptimizer\n  10.91MiB from inputs/Cast\n  Remaining 3 nodes with 768B\n\n\t [[Node: layer_c_1.3c_branch_4_conv_1.4/bn_c_1.3c_branch_4_conv_1.4/AssignMovingAvg_1/_1281 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_29840...ovingAvg_1\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCurrent usage from device: /job:localhost/replica:0/task:0/device:GPU:0, allocator: GPU_0_bfc\n  86.72MiB from layer_stem_1.2/bn_stem_1.2/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/bn_stem_1.1/cond/FusedBatchNorm\n  86.72MiB from layer_stem_1.1/conv_stem_1.1/Conv2D\n  86.72MiB from gradients/AddN_470-0-TransposeNHWCToNCHW-LayoutOptimizer\n  84.41MiB from layer_stem_1.2/conv_stem_1.2/Conv2D\n  84.41MiB from gradients/zeros_884-0-1-TransposeNCHWToNHWC-LayoutOptimizer\n  10.91MiB from inputs/Cast\n  Remaining 3 nodes with 768B\n\n"
     ]
    }
   ],
   "source": [
    "## train the model\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    if log_to_tensorboard:\n",
    "        train_writer = tf.summary.FileWriter('./logs/tr_' + model_name, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./logs/te_' + model_name)\n",
    "\n",
    "    if not print_metrics:\n",
    "        # create a plot to be updated as model is trained\n",
    "        f, ax = plt.subplots(1, 4, figsize=(24, 5))\n",
    "\n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # If the model is new initialize variables, else restore the session\n",
    "    if init:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Initializing model...\")\n",
    "    else:\n",
    "        # if we are initializing with the weights from another model load it\n",
    "        if init_model is not None:\n",
    "            # initialize the global variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # create the initializer function to initialize the weights\n",
    "            init_fn = load_weights(init_model, exclude=[\"conv5\", \"bn5\", \"fc1\", \"fc1_bb\", \"fc2_bn\", \"fc3\", \"fc3_bn\", \"fc2\", \"fc_logits\", \"global_step\"])\n",
    "\n",
    "            # run the initializer\n",
    "            init_fn(sess)\n",
    "\n",
    "            # reset the global step\n",
    "            initial_global_step = global_step.assign(0)\n",
    "            sess.run(initial_global_step)\n",
    "\n",
    "            print(\"Initializing weights from model\", init_model)\n",
    "\n",
    "            # reset init model so we don't do this again\n",
    "            init_model = None\n",
    "        # otherwise load this model\n",
    "        else:\n",
    "            saver.restore(sess, './model/' + model_name + '.ckpt')\n",
    "            print(\"Restoring model\", model_name)\n",
    "\n",
    "    # if we are training the model\n",
    "    if action == \"train\":\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        print(\"Training model\", model_name, \"...\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            # Accuracy values (train) after each batch\n",
    "            batch_acc = []\n",
    "            batch_cost = []\n",
    "            batch_recall = []\n",
    "\n",
    "            for i in range(steps_per_epoch):\n",
    "                # create the metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "\n",
    "                # Run training op and update ops\n",
    "                if (i % 50 != 0) or (i == 0):\n",
    "                    # log the kernel images once per epoch\n",
    "                    if (i == (steps_per_epoch - 1)) and log_to_tensorboard:\n",
    "                        _, _, _, image_summary, step = sess.run(\n",
    "                            [train_op, extra_update_ops, update_op, kernel_summaries, global_step],\n",
    "                            feed_dict={\n",
    "                                training: True,\n",
    "                            },\n",
    "                            options=run_options,\n",
    "                            run_metadata=run_metadata)\n",
    "\n",
    "                        # write the summary\n",
    "                        train_writer.add_summary(image_summary, step)\n",
    "                    else:\n",
    "                        _, _, _, step = sess.run(\n",
    "                            [train_op, extra_update_ops, update_op, global_step],\n",
    "                                feed_dict={\n",
    "                                    training: True,\n",
    "                                },\n",
    "                                options=run_options,\n",
    "                                run_metadata=run_metadata)\n",
    "\n",
    "                # every 50th step get the metrics\n",
    "                else:\n",
    "                    _, _, _, precision_value, summary, acc_value, cost_value, recall_value, step, lr = sess.run(\n",
    "                        [train_op, extra_update_ops, update_op, prec_op, merged, accuracy, mean_ce, rec_op, global_step, learning_rate],\n",
    "                        feed_dict={\n",
    "                            training: True,\n",
    "                        },\n",
    "                        options=run_options,\n",
    "                        run_metadata=run_metadata)\n",
    "\n",
    "                    # Save accuracy (current batch)\n",
    "                    batch_acc.append(acc_value)\n",
    "                    batch_cost.append(cost_value)\n",
    "                    batch_recall.append(recall_value)\n",
    "\n",
    "                    # log the summaries to tensorboard every 50 steps\n",
    "                    if log_to_tensorboard:\n",
    "                        # write the summary\n",
    "                        train_writer.add_summary(summary, step)\n",
    "\n",
    "                # only log the meta data once per epoch\n",
    "                if i == 1:\n",
    "                    train_writer.add_run_metadata(run_metadata, 'step %d' % step)\n",
    "\n",
    "            # save checkpoint every nth epoch\n",
    "            if (epoch % checkpoint_every == 0):\n",
    "                print(\"Saving checkpoint\")\n",
    "                save_path = saver.save(sess, './model/' + model_name + '.ckpt')\n",
    "\n",
    "                # Now that model is saved set init to false so we reload it next time\n",
    "                init = False\n",
    "\n",
    "            # init batch arrays\n",
    "            batch_cv_acc = []\n",
    "            batch_cv_loss = []\n",
    "            batch_cv_recall = []\n",
    "\n",
    "            # initialize the local variables so we have metrics only on the evaluation\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            print(\"Evaluating model...\")\n",
    "            # load the test data\n",
    "            X_cv, y_cv = load_validation_data(percentage=1, how=how, which=dataset)\n",
    "\n",
    "            # evaluate the test data\n",
    "            for X_batch, y_batch in get_batches(X_cv, y_cv, batch_size, distort=False):\n",
    "                _, _, valid_acc, valid_recall, valid_precision, valid_fscore, valid_cost = sess.run(\n",
    "                    [update_op, extra_update_ops, accuracy, rec_op, prec_op, f1_score, mean_ce],\n",
    "                    feed_dict={\n",
    "                        X: X_batch,\n",
    "                        y: y_batch,\n",
    "                        training: False\n",
    "                    })\n",
    "\n",
    "                batch_cv_acc.append(valid_acc)\n",
    "                batch_cv_loss.append(valid_cost)\n",
    "                batch_cv_recall.append(valid_recall)\n",
    "\n",
    "            # Write average of validation data to summary logs\n",
    "            if log_to_tensorboard:\n",
    "                # evaluate once more to get the summary, which will then be written to tensorboard\n",
    "                summary, cv_accuracy = sess.run(\n",
    "                    [merged, accuracy],\n",
    "                    feed_dict={\n",
    "                        X: X_cv[0:2],\n",
    "                        y: y_cv[0:2],\n",
    "                        training: False\n",
    "                    })\n",
    "\n",
    "            test_writer.add_summary(summary, step)\n",
    "            # test_writer.add_summary(other_summaries, step)\n",
    "            step += 1\n",
    "\n",
    "            # delete the test data to save memory\n",
    "            del (X_cv)\n",
    "            del (y_cv)\n",
    "\n",
    "            print(\"Done evaluating...\")\n",
    "\n",
    "            # take the mean of the values to add to the metrics\n",
    "            valid_acc_values.append(np.mean(batch_cv_acc))\n",
    "            train_acc_values.append(np.mean(batch_acc))\n",
    "\n",
    "            valid_cost_values.append(np.mean(batch_cv_loss))\n",
    "            train_cost_values.append(np.mean(batch_cost))\n",
    "\n",
    "            valid_recall_values.append(np.mean(batch_cv_recall))\n",
    "            train_recall_values.append(np.mean(batch_recall))\n",
    "\n",
    "            train_lr_values.append(lr)\n",
    "\n",
    "            # save the metrics\n",
    "            np.save(os.path.join(\"data\", model_name + \"train_acc.npy\"), train_acc_values)\n",
    "            np.save(os.path.join(\"data\", model_name + \"cv_acc.npy\"), valid_acc_values)\n",
    "\n",
    "            np.save(os.path.join(\"data\", model_name + \"train_loss.npy\"), train_cost_values)\n",
    "            np.save(os.path.join(\"data\", model_name + \"cv_loss.npy\"), valid_cost_values)\n",
    "\n",
    "            np.save(os.path.join(\"data\", model_name + \"train_recall.npy\"), train_recall_values)\n",
    "            np.save(os.path.join(\"data\", model_name + \"cv_recall.npy\"), valid_recall_values)\n",
    "\n",
    "            np.save(os.path.join(\"data\", model_name + \"train_lr.npy\"), train_lr_values)\n",
    "\n",
    "            # Print progress every nth epoch to keep output to reasonable amount\n",
    "            if (epoch % print_every == 0):\n",
    "                print(\n",
    "                'Epoch {:02d} - step {} - cv acc: {:.4f} - train acc: {:.3f} (mean)'.format(\n",
    "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc)\n",
    "                ))\n",
    "\n",
    "            # Print data every 50th epoch so I can write it down to compare models\n",
    "            if (not print_metrics) and (epoch % 50 == 0) and (epoch > 1):\n",
    "                if (epoch % print_every == 0):\n",
    "                    print(\n",
    "                    'Epoch {:02d} - step {} - cv acc: {:.4f} - train acc: {:.3f} (mean)'.format(\n",
    "                        epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc)\n",
    "                    ))\n",
    "\n",
    "        # stop the coordinator\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to stop\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
