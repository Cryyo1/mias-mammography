{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is model 1.0.0.29 trained on dataset 8 classifying to all five categories. The model was actually trained on a Google Cloud instance with GPU as it was far too computationally expensive to train on my laptop. When training the metrics were saved to disk and those are used for the plots below. Metrics were also logged to TensorBoard. Those logs as well as the resulting model are included in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import wget\n",
    "import tensorflow as tf\n",
    "from training_utils import download_file, get_batches, read_and_decode_single_example, load_validation_data, \\\n",
    "    download_data, evaluate_model, get_training_data, load_weights, flatten\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import argparse\n",
    "from tensorboard import summary as summary_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# arguments\n",
    "epochs = 35\n",
    "dataset = 9\n",
    "init_model = None\n",
    "how = \"normal\"\n",
    "action = \"train\"\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the data\n",
    "#download_data(what=dataset)\n",
    "batch_size = 32\n",
    "train_files, total_records = get_training_data(what=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 1366\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "epsilon = 1e-8\n",
    "\n",
    "# learning rate\n",
    "epochs_per_decay = 10\n",
    "starting_rate = 0.001\n",
    "decay_factor = 0.80\n",
    "staircase = True\n",
    "\n",
    "# learning rate decay variables\n",
    "steps_per_epoch = int(total_records / batch_size)\n",
    "print(\"Steps per epoch:\", steps_per_epoch)\n",
    "\n",
    "# lambdas\n",
    "lamC = 0.00001\n",
    "lamF = 0.00250\n",
    "\n",
    "mu = 104\n",
    "\n",
    "# use dropout\n",
    "dropout = True\n",
    "fcdropout_rate = 0.5\n",
    "convdropout_rate = 0.001\n",
    "pooldropout_rate = 0.1\n",
    "\n",
    "if how == \"label\":\n",
    "    num_classes = 5\n",
    "elif how == \"normal\":\n",
    "    num_classes = 2\n",
    "elif how == \"mass\":\n",
    "    num_classes = 3\n",
    "elif how == \"benign\":\n",
    "    num_classes = 3\n",
    "\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Graph created...\n"
     ]
    }
   ],
   "source": [
    "model_name = \"model_s1.0.0.29l.14\"\n",
    "## Change Log\n",
    "# 0.0.0.4 - increase pool3 to 3x3 with stride 3\n",
    "# 0.0.0.6 - reduce pool 3 stride back to 2\n",
    "# 0.0.0.7 - reduce lambda for l2 reg\n",
    "# 0.0.0.8 - increase conv1 to 7x7 stride 2\n",
    "# 0.0.0.9 - disable per image normalization\n",
    "# 0.0.0.10 - commented out batch norm in conv layers, added conv4 and changed stride of convs to 1, increased FC lambda\n",
    "# 0.0.0.11 - turn dropout for conv layers on\n",
    "# 0.0.0.12 - added batch norm after pooling layers, increase pool dropout, decrease conv dropout, added extra conv layer to reduce data dimensionality\n",
    "# 0.0.0.13 - added precision and f1 summaries\n",
    "# 0.0.0.14 - fixing batch normalization, I don't think it's going to work after each pool\n",
    "# 0.0.0.15 - reduced xentropy weighting term\n",
    "# 0.0.0.17 - replaced initial 5x5 conv layers with 3 3x3 layers\n",
    "# 0.0.0.18 - changed stride of first conv to 2 from 1\n",
    "# 0.0.0.19 - doubled units in two fc layers\n",
    "# 0.0.0.20 - lowered learning rate, put a batch norm back in\n",
    "# 0.0.0.21 - put all batch norms back in\n",
    "# 0.0.0.22 - increased lambdaC, removed dropout from conv layers\n",
    "# 1.0.0.23 - added extra conv layers\n",
    "# 1.0.0.27 - updates to training code and metrics\n",
    "# 1.0.0.28 - using weighted x-entropy to improve recall\n",
    "# 1.0.0.29 - updated code to work training to classify for multiple classes\n",
    "# 1.0.0.29f - putting weighted x-entropy back\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    training = tf.placeholder(dtype=tf.bool, name=\"is_training\")\n",
    "    is_testing = tf.placeholder(dtype=bool, shape=(), name=\"is_testing\")\n",
    "\n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(starting_rate,\n",
    "                                               global_step,\n",
    "                                               steps_per_epoch * epochs_per_decay,\n",
    "                                               decay_factor,\n",
    "                                               staircase=staircase)\n",
    "\n",
    "    with tf.name_scope('inputs') as scope:\n",
    "        image, label = read_and_decode_single_example(train_files, label_type=how, normalize=False)\n",
    "\n",
    "        X_def, y_def = tf.train.shuffle_batch([image, label], batch_size=batch_size, capacity=2000,\n",
    "                                              min_after_dequeue=1000)\n",
    "\n",
    "        # Placeholders\n",
    "        X = tf.placeholder_with_default(X_def, shape=[None, 299, 299, 1])\n",
    "        y = tf.placeholder_with_default(y_def, shape=[None])\n",
    "        \n",
    "        X_adj = tf.image.adjust_contrast(X, 2.0)\n",
    "        X_adj = tf.cast(X_adj, dtype=tf.float32)\n",
    "\n",
    "        # center the pixel data\n",
    "        mu = tf.constant(mu, name=\"pixel_mean\", dtype=tf.float32)\n",
    "        X_adj = tf.subtract(X_adj, mu, name=\"centered_input\")\n",
    "\n",
    "        # scale the data\n",
    "        X_adj = tf.divide(X_adj, 255.0)\n",
    "        \n",
    "    # Convolutional layer 1\n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X_adj,  # Input data\n",
    "            filters=32,\n",
    "            kernel_size=(3, 3),\n",
    "            strides=(2, 2),\n",
    "            padding='SAME',\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=100),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv1'\n",
    "        )\n",
    "\n",
    "        conv1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv1 = tf.nn.relu(conv1, name='relu1')\n",
    "        \n",
    "    print(\"Graph created...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CONFIGURE OPTIONS\n",
    "# if we are initializing the weights from a pre-trained model make sure it exists\n",
    "if init_model is not None:\n",
    "    if os.path.exists(os.path.join(\"model\", init_model + '.ckpt.index')):\n",
    "        init = False\n",
    "    else:\n",
    "        init = True\n",
    "# otherwise if this model exists let's continue training it rather than reinitializing it\n",
    "else:\n",
    "    if os.path.exists(os.path.join(\"model\", model_name + '.ckpt.index')):\n",
    "        init = False\n",
    "    else:\n",
    "        init = True\n",
    "\n",
    "meta_data_every = 1\n",
    "log_to_tensorboard = True\n",
    "print_every = 5  # how often to print metrics\n",
    "checkpoint_every = 1  # how often to save model in epochs\n",
    "use_gpu = False  # whether or not to use the GPU\n",
    "print_metrics = True  # whether to print or plot metrics, if False a plot will be created and updated every epoch\n",
    "\n",
    "config = tf.ConfigProto(device_count = {'GPU': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Training model model_s1.0.0.29l.14 ...\n",
      "Train Mean: 0.009218497\n",
      "Train Min: -0.40784317\n",
      "Train Max: 0.5921569\n",
      "Train C1 Mean: 0.3600788\n",
      "Train C1 Min: 0.0\n",
      "Train C1 Max: 12.572195\n",
      "Evaluating model...\n",
      "CV Mean: 0.008187771\n",
      "CV Min: -0.40784317\n",
      "CV Max: 0.5921569\n",
      "CV C1 Mean: 0.0068575907\n",
      "CV C1 Min: 0.0\n",
      "CV C1 Max: 0.16763291\n",
      "Done evaluating...\n"
     ]
    }
   ],
   "source": [
    "init = True\n",
    "## train the model\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    if log_to_tensorboard:\n",
    "        train_writer = tf.summary.FileWriter('./logs/tr_' + model_name, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./logs/te_' + model_name)\n",
    "\n",
    "    if not print_metrics:\n",
    "        # create a plot to be updated as model is trained\n",
    "        f, ax = plt.subplots(1, 4, figsize=(24, 5))\n",
    "\n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initializing model...\")\n",
    "\n",
    "    # if we are training the model\n",
    "    if action == \"train\":\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        print(\"Training model\", model_name, \"...\")\n",
    "\n",
    "        for epoch in range(2):\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            for i in range(steps_per_epoch):\n",
    "                # create the metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "\n",
    "            \n",
    "                image_batch, conv1_batch = sess.run(\n",
    "                    [X_adj, conv1],\n",
    "                    feed_dict={\n",
    "                        training: True,\n",
    "                    },\n",
    "                    options=run_options,\n",
    "                    run_metadata=run_metadata)\n",
    "                \n",
    "                image_arr = np.array(image_batch)\n",
    "                conv1_batch = np.array(conv1_batch)\n",
    "#                 plt.imshow(image_arr[0].reshape([299,299]))\n",
    "#                 plt.show()\n",
    "                print(\"Train Mean:\", np.mean(image_arr))\n",
    "                print(\"Train Min:\", np.min(image_arr))\n",
    "                print(\"Train Max:\", np.max(image_arr))\n",
    "            \n",
    "                print(\"Train C1 Mean:\", np.mean(conv1_batch))\n",
    "                print(\"Train C1 Min:\", np.min(conv1_batch))\n",
    "                print(\"Train C1 Max:\", np.max(conv1_batch))\n",
    "                break\n",
    "            \n",
    "            # initialize the local variables so we have metrics only on the evaluation\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            print(\"Evaluating model...\")\n",
    "            # load the test data\n",
    "            X_cv, y_cv = load_validation_data(percentage=1, how=how, which=dataset)\n",
    "\n",
    "            # evaluate the test data\n",
    "            for X_batch, y_batch in get_batches(X_cv, y_cv, batch_size, distort=False):\n",
    "                cv_image_batch, cv_conv1_batch = sess.run(\n",
    "                    [X_adj, conv1],\n",
    "                    feed_dict={\n",
    "                        X: X_batch,\n",
    "                        y: y_batch,\n",
    "                        training: False\n",
    "                    })\n",
    "                break\n",
    "            \n",
    "            cv_image_arr = np.array(cv_image_batch)\n",
    "            cv_conv1_batch = np.array(cv_conv1_batch)\n",
    "#             plt.imshow(cv_image_arr[0].reshape([299,299]))\n",
    "#             plt.show()\n",
    "            print(\"CV Mean:\", np.mean(cv_image_arr))\n",
    "            print(\"CV Min:\", np.min(cv_image_arr))\n",
    "            print(\"CV Max:\", np.max(cv_image_arr))\n",
    "            \n",
    "            print(\"CV C1 Mean:\", np.mean(cv_conv1_batch))\n",
    "            print(\"CV C1 Min:\", np.min(cv_conv1_batch))\n",
    "            print(\"CV C1 Max:\", np.max(cv_conv1_batch))\n",
    "            \n",
    "            # delete the test data to save memory\n",
    "            del (X_cv)\n",
    "            del (y_cv)\n",
    "\n",
    "            print(\"Done evaluating...\")\n",
    "\n",
    "            break\n",
    "        # stop the coordinator\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to stop\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_image = image_arr[0]\n",
    "cv_image = cv_image_arr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40100044\n",
      "0.047194194\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_image))\n",
    "print(np.mean(cv_image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
