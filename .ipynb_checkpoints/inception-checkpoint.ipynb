{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import wget\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from training_utils import download_file, get_batches, read_and_decode_single_example, load_validation_data, \\\n",
    "    download_data, evaluate_model, get_training_data, load_weights, flatten, _conv2d_batch_norm\n",
    "from inception_utils import _stem, _block_a, _block_b, _block_c, _reduce_a, _reduce_b\n",
    "import argparse\n",
    "from tensorboard import summary as summary_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "dataset = 9\n",
    "init_model = None\n",
    "how = \"normal\"\n",
    "action = \"train\"\n",
    "threshold = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 1366\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "# precalculated pixel mean of images\n",
    "mu = 104.1353\n",
    "\n",
    "# download the data\n",
    "download_data(what=dataset)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_files, total_records = get_training_data(what=dataset)\n",
    "\n",
    "## Hyperparameters\n",
    "# Small epsilon value for the BN transform\n",
    "epsilon = 1e-8\n",
    "\n",
    "# learning rate\n",
    "epochs_per_decay = 10\n",
    "starting_rate = 0.001\n",
    "decay_factor = 0.80\n",
    "staircase = True\n",
    "\n",
    "# learning rate decay variables\n",
    "steps_per_epoch = int(total_records / batch_size)\n",
    "print(\"Steps per epoch:\", steps_per_epoch)\n",
    "\n",
    "# lambdas\n",
    "lamC = 0.00000\n",
    "lamF = 0.00250\n",
    "\n",
    "# use dropout\n",
    "dropout = False\n",
    "fcdropout_rate = 0.5\n",
    "convdropout_rate = 0.0\n",
    "pooldropout_rate = 0.0\n",
    "\n",
    "if how == \"label\":\n",
    "    num_classes = 5\n",
    "elif how == \"normal\":\n",
    "    num_classes = 2\n",
    "elif how == \"mass\":\n",
    "    num_classes = 3\n",
    "elif how == \"benign\":\n",
    "    num_classes = 3\n",
    "\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "Predictions: (?, 1, 2)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shapes (?, 1, 2) and (?,) are incompatible",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-21d05a8e1908>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mupdates_collections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mUPDATE_OPS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m         \u001b[1;31m# metrics_collections=[\"summaries\"],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m         \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    127\u001b[0m     )\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py\u001b[0m in \u001b[0;36maccuracy\u001b[1;34m(labels, predictions, weights, metrics_collections, updates_collections, name)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m   predictions, labels, weights = _remove_squeezable_dimensions(\n\u001b[1;32m--> 402\u001b[1;33m       predictions=predictions, labels=labels, weights=weights)\n\u001b[0m\u001b[0;32m    403\u001b[0m   \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\ops\\metrics_impl.py\u001b[0m in \u001b[0;36m_remove_squeezable_dimensions\u001b[1;34m(predictions, labels, weights)\u001b[0m\n\u001b[0;32m     78\u001b[0m     labels, predictions = confusion_matrix.remove_squeezable_dimensions(\n\u001b[0;32m     79\u001b[0m         labels, predictions)\n\u001b[1;32m---> 80\u001b[1;33m     \u001b[0mpredictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \"\"\"\n\u001b[0;32m    763\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shapes (?, 1, 2) and (?,) are incompatible"
     ]
    }
   ],
   "source": [
    "## Build the graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "# whether to retrain model from scratch or use saved model\n",
    "init = True\n",
    "model_name = \"vgg_16.3.01l.6\"\n",
    "# vgg_19.01 - attempting to recreate vgg 19 architecture\n",
    "# vgg_16.02 - went to vgg 16 architecture, reducing units in fc layers\n",
    "# vgg_16.2.01 - changing first conv layers to stride 2 to get dimensions down to reasonable size\n",
    "# vgg_16.2.02 - using normal x-entropy instead of weighted\n",
    "# vgg_16.3.01 - average pooling image before first conv, changing conv1 to stride 1\n",
    "\n",
    "with graph.as_default():\n",
    "    training = tf.placeholder(dtype=tf.bool, name=\"is_training\")\n",
    "    is_testing = tf.placeholder(dtype=bool, shape=(), name=\"is_testing\")\n",
    "\n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(starting_rate,\n",
    "                                               global_step,\n",
    "                                               steps_per_epoch * epochs_per_decay,\n",
    "                                               decay_factor,\n",
    "                                               staircase=staircase)\n",
    "\n",
    "    with tf.name_scope('inputs') as scope:\n",
    "        image, label = read_and_decode_single_example(train_files, label_type=how, normalize=False)\n",
    "\n",
    "        X_def, y_def = tf.train.shuffle_batch([image, label], batch_size=batch_size, capacity=2000,\n",
    "                                              min_after_dequeue=1000)\n",
    "\n",
    "        # Placeholders\n",
    "        X = tf.placeholder_with_default(X_def, shape=[None, 299, 299, 1])\n",
    "        y = tf.placeholder_with_default(y_def, shape=[None])\n",
    "\n",
    "        X = tf.cast(X, dtype=tf.float32)\n",
    "\n",
    "        # center the pixel data\n",
    "        mu = tf.constant(mu, name=\"pixel_mean\", dtype=tf.float32)\n",
    "        X = tf.subtract(X, mu, name=\"centered_input\")\n",
    "\n",
    "    # input stem\n",
    "    stem = _stem(X, lamC, training)\n",
    "\n",
    "    # 4 Block As\n",
    "    blocka = _block_a(stem, name=\"a_1.1\", lamC=0.0, training = training)\n",
    "    blocka = _block_a(blocka, name=\"a_1.2\", lamC=0.0, training=training)\n",
    "    blocka = _block_a(blocka, name=\"a_1.3\", lamC=0.0, training=training)\n",
    "    blocka = _block_a(blocka, name=\"a_1.4\", lamC=0.0, training=training)\n",
    "\n",
    "    # Reduction A\n",
    "    reducea = _reduce_a(blocka, \"a_reduce_1\", k=192, l=224, m=256, n=384, training = training)\n",
    "\n",
    "    # 7 Block Bs\n",
    "    blockb = _block_b(reducea, \"b_1.1\", lamC=0.0, training = training)\n",
    "    blockb = _block_b(blockb, \"b_1.2\", lamC=0.0, training=training)\n",
    "    blockb = _block_b(blockb, \"b_1.3\", lamC=0.0, training=training)\n",
    "    blockb = _block_b(blockb, \"b_1.4\", lamC=0.0, training=training)\n",
    "    blockb = _block_b(blockb, \"b_1.5\", lamC=0.0, training=training)\n",
    "    blockb = _block_b(blockb, \"b_1.6\", lamC=0.0, training=training)\n",
    "    blockb = _block_b(blockb, \"b_1.7\", lamC=0.0, training=training)\n",
    "\n",
    "    # Reduction B\n",
    "    reduceb = _reduce_b(blockb, name=\"b_reduce_1\", training = training)\n",
    "\n",
    "    # 3 Block Cs\n",
    "    blockc = _block_c(reduceb, name=\"c_1.1\", lamC=0.0, training = training)\n",
    "    blockc = _block_c(blockc, name=\"c_1.2\", lamC=0.0, training=training)\n",
    "    blockc = _block_c(blockc, name=\"c_1.3\", lamC=0.0, training=training)\n",
    "\n",
    "    # Global Average Pooling\n",
    "    global_pool = tf.layers.average_pooling2d(\n",
    "            blockc,  # Input\n",
    "            pool_size=(8, 8),  # Pool size: 2x2\n",
    "            strides=(8, 8),  # Stride: 2\n",
    "            padding='SAME',  # \"same\" padding\n",
    "            name='global_pool'\n",
    "        )\n",
    "\n",
    "    global_pool = tf.layers.dropout(global_pool, rate=0.2, seed=103, training=training)\n",
    "    \n",
    "    # flatten the output\n",
    "    flat_output = tf.contrib.layers.flatten(global_pool)\n",
    "    \n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        flat_output,\n",
    "        num_classes,  # One output unit per category\n",
    "        activation=None,  # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=121),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name=\"fc_logits\"\n",
    "    )\n",
    "\n",
    "    # get the fully connected variables so we can only train them when retraining the network\n",
    "    fc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"fc\")\n",
    "\n",
    "    # with tf.variable_scope('conv_1.1', reuse=True):\n",
    "    #     conv_kernels1 = tf.get_variable('kernel')\n",
    "    #     kernel_transposed = tf.transpose(conv_kernels1, [3, 0, 1, 2])\n",
    "    #\n",
    "    # with tf.variable_scope('visualization'):\n",
    "    #     tf.summary.image('conv_1.1/filters', kernel_transposed, max_outputs=32, collections=[\"kernels\"])\n",
    "\n",
    "    # get the probabilites for the classes\n",
    "    probabilities = tf.nn.softmax(logits, name=\"probabilities\")\n",
    "\n",
    "    # the probability that the scan is abnormal is 1 - probability it is normal\n",
    "    abnormal_probability = (1 - probabilities[:, 0])\n",
    "\n",
    "    if num_classes > 2:\n",
    "        # the scan is abnormal if the probability is greater than the threshold\n",
    "        #is_abnormal = tf.cast(tf.greater(abnormal_probability, threshold), tf.int64)\n",
    "\n",
    "        # Compute predictions from the probabilities - if the scan is normal we ignore the other probabilities\n",
    "        #predictions = is_abnormal * tf.argmax(probabilities[:,1:], axis=1, output_type=tf.int64)\n",
    "        predictions = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
    "    else:\n",
    "        predictions = tf.cast(tf.greater(abnormal_probability, threshold), tf.int32)\n",
    "    \n",
    "    print(\"Predictions:\", predictions.shape)\n",
    "    \n",
    "    # get the accuracy\n",
    "    accuracy, acc_op = tf.metrics.accuracy(\n",
    "        labels=y,\n",
    "        predictions=predictions,\n",
    "        updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
    "        # metrics_collections=[\"summaries\"],\n",
    "        name=\"accuracy\",\n",
    "    )\n",
    "\n",
    "    #########################################################\n",
    "    ## Loss function options\n",
    "    # Regular mean cross entropy\n",
    "    mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "    #########################################################\n",
    "    ## Weight the positive examples higher\n",
    "    # This will weight the positive examples higher so as to improve recall\n",
    "    #weights = tf.multiply(1, tf.cast(tf.greater(y, 0), tf.int32)) + 1\n",
    "    #mean_ce = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits, weights=weights))\n",
    "\n",
    "    # Add in l2 loss\n",
    "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    # Minimize cross-entropy\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # calculate recall\n",
    "    if num_classes > 2:\n",
    "        # collapse the predictions down to normal or not for our pr metrics\n",
    "        zero = tf.constant(0, dtype=tf.int64)\n",
    "        collapsed_predictions = tf.cast(tf.greater(predictions, zero), tf.int64)\n",
    "        collapsed_labels = tf.greater(y, zero)\n",
    "\n",
    "        recall, rec_op = tf.metrics.recall(labels=collapsed_labels, predictions=collapsed_predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"recall\")\n",
    "        precision, prec_op = tf.metrics.precision(labels=collapsed_labels, predictions=collapsed_predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"precision\")\n",
    "\n",
    "    else:\n",
    "        recall, rec_op = tf.metrics.recall(labels=y, predictions=predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"recall\")\n",
    "        precision, prec_op = tf.metrics.precision(labels=y, predictions=predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"precision\")\n",
    "\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "    _, update_op = summary_lib.pr_curve_streaming_op(name='pr_curve',\n",
    "                                                     predictions=(1 - probabilities[:, 0]),\n",
    "                                                     labels=y,\n",
    "                                                     updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
    "                                                     num_thresholds=20)\n",
    "\n",
    "    tf.summary.scalar('recall_1', recall, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('precision_1', precision, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('f1_score', f1_score, collections=[\"summaries\"])\n",
    "\n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('cross_entropy', mean_ce, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('learning_rate', learning_rate, collections=[\"summaries\"])\n",
    "\n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    # Merge all the summaries\n",
    "    merged = tf.summary.merge_all(\"summaries\")\n",
    "    kernel_summaries = tf.summary.merge_all(\"kernels\")\n",
    "    per_epoch_summaries = [[]]\n",
    "\n",
    "    print(\"Graph created...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
