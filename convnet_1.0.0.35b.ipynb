{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is model 1.0.0.35 doing binary classification. The model was actually trained on a Google Cloud instance with GPU as it was far too computationally expensive to train on my laptop. When training the metrics were saved to disk and those are used for the plots below. Metrics were also logged to TensorBoard. Those logs are included in the repository and the model checkpoint can be downloaded from S3 here: https://s3.eu-central-1.amazonaws.com/aws.skoo.ch/files/model_s1.0.0.35b.98.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import wget\n",
    "import tensorflow as tf\n",
    "from training_utils import download_file, get_batches, read_and_decode_single_example, load_validation_data, \\\n",
    "    download_data, evaluate_model, get_training_data, load_weights, flatten, augment, _scale_input_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#import argparse\n",
    "from tensorboard import summary as summary_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# arguments\n",
    "epochs = 40\n",
    "dataset = 9\n",
    "init_model = None\n",
    "how = \"normal\"\n",
    "action = \"train\"\n",
    "threshold = 0.5\n",
    "contrast = 1\n",
    "distort = False\n",
    "weight = 6\n",
    "freeze = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download the data\n",
    "#download_data(what=dataset)\n",
    "batch_size = 32\n",
    "train_files, total_records = get_training_data(what=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps per epoch: 1366\n",
      "Number of classes: 2\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameters\n",
    "epsilon = 1e-8\n",
    "\n",
    "# learning rate\n",
    "epochs_per_decay = 5\n",
    "starting_rate = 0.001\n",
    "decay_factor = 0.80\n",
    "staircase = True\n",
    "\n",
    "# learning rate decay variables\n",
    "steps_per_epoch = int(total_records / batch_size)\n",
    "print(\"Steps per epoch:\", steps_per_epoch)\n",
    "\n",
    "# lambdas\n",
    "lamC = 0.00001\n",
    "lamF = 0.00250\n",
    "\n",
    "# use dropout\n",
    "dropout = True\n",
    "fcdropout_rate = 0.5\n",
    "convdropout_rate = 0.001\n",
    "pooldropout_rate = 0.1\n",
    "\n",
    "if how == \"label\":\n",
    "    num_classes = 5\n",
    "elif how == \"normal\":\n",
    "    num_classes = 2\n",
    "elif how == \"mass\":\n",
    "    num_classes = 3\n",
    "elif how == \"benign\":\n",
    "    num_classes = 3\n",
    "\n",
    "print(\"Number of classes:\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\eric\\Anaconda2\\envs\\exts-aml2\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "Graph created...\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "model_name = \"model_s1.0.0.35b.98\"\n",
    "## Change Log\n",
    "# 0.0.0.4 - increase pool3 to 3x3 with stride 3\n",
    "# 0.0.0.6 - reduce pool 3 stride back to 2\n",
    "# 0.0.0.7 - reduce lambda for l2 reg\n",
    "# 0.0.0.8 - increase conv1 to 7x7 stride 2\n",
    "# 0.0.0.9 - disable per image normalization\n",
    "# 0.0.0.10 - commented out batch norm in conv layers, added conv4 and changed stride of convs to 1, increased FC lambda\n",
    "# 0.0.0.11 - turn dropout for conv layers on\n",
    "# 0.0.0.12 - added batch norm after pooling layers, increase pool dropout, decrease conv dropout, added extra conv layer to reduce data dimensionality\n",
    "# 0.0.0.13 - added precision and f1 summaries\n",
    "# 0.0.0.14 - fixing batch normalization, I don't think it's going to work after each pool\n",
    "# 0.0.0.15 - reduced xentropy weighting term\n",
    "# 0.0.0.17 - replaced initial 5x5 conv layers with 3 3x3 layers\n",
    "# 0.0.0.18 - changed stride of first conv to 2 from 1\n",
    "# 0.0.0.19 - doubled units in two fc layers\n",
    "# 0.0.0.20 - lowered learning rate, put a batch norm back in\n",
    "# 0.0.0.21 - put all batch norms back in\n",
    "# 0.0.0.22 - increased lambdaC, removed dropout from conv layers\n",
    "# 1.0.0.23 - added extra conv layers\n",
    "# 1.0.0.27 - updates to training code and metrics\n",
    "# 1.0.0.28 - using weighted x-entropy to improve recall\n",
    "# 1.0.0.29 - updated code to work training to classify for multiple classes\n",
    "# 1.0.0.29f - putting weighted x-entropy back\n",
    "# 1.0.0.30b - changed some hyperparameters\n",
    "# 1.0.0.31l - added decision threshold to predictions\n",
    "# 1.0.0.33 - scaling input data\n",
    "# 1.0.0.34 - centering data by 127, not by mean\n",
    "# 1.0.0.35 - not centering data, just scaling it\n",
    "\n",
    "with graph.as_default():\n",
    "    training = tf.placeholder(dtype=tf.bool, name=\"is_training\")\n",
    "    is_testing = tf.placeholder(dtype=bool, shape=(), name=\"is_testing\")\n",
    "\n",
    "    # create global step for decaying learning rate\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(starting_rate,\n",
    "                                               global_step,\n",
    "                                               steps_per_epoch * epochs_per_decay,\n",
    "                                               decay_factor,\n",
    "                                               staircase=staircase)\n",
    "\n",
    "    with tf.name_scope('inputs') as scope:\n",
    "        image, label = read_and_decode_single_example(train_files, label_type=how, normalize=False, distort=False)\n",
    "\n",
    "        X_def, y_def = tf.train.shuffle_batch([image, label], batch_size=batch_size, capacity=2000,\n",
    "                                              seed=None,\n",
    "                                              min_after_dequeue=1000)\n",
    "\n",
    "        # Placeholders\n",
    "        X = tf.placeholder_with_default(X_def, shape=[None, 299, 299, 1])\n",
    "        y = tf.placeholder_with_default(y_def, shape=[None])\n",
    "\n",
    "        #X = tf.cast(X, dtype=tf.float32)\n",
    "        X_adj = _scale_input_data(X, contrast=contrast, mu=0, scale=255.0)\n",
    "\n",
    "        # data augmentation\n",
    "        if distort:\n",
    "            X_adj, y = augment(X_adj, y, horizontal_flip=True, vertical_flip=True, mixup=0)\n",
    "\n",
    "    # Convolutional layer 1\n",
    "    with tf.name_scope('conv1') as scope:\n",
    "        conv1 = tf.layers.conv2d(\n",
    "            X_adj,  # Input data\n",
    "            filters=32,  # 32 filters\n",
    "            kernel_size=(3, 3),  # Kernel size: 5x5\n",
    "            strides=(2, 2),  # Stride: 2\n",
    "            padding='SAME',  # \"same\" padding\n",
    "            activation=None,  # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=100),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv1'\n",
    "        )\n",
    "\n",
    "        conv1 = tf.layers.batch_normalization(\n",
    "            conv1,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv1_bn_relu = tf.nn.relu(conv1, name='relu1')\n",
    "\n",
    "    with tf.name_scope('conv1.1') as scope:\n",
    "        conv11 = tf.layers.conv2d(\n",
    "            conv1_bn_relu,  # Input data\n",
    "            filters=32,  # 32 filters\n",
    "            kernel_size=(3, 3),  # Kernel size: 5x5\n",
    "            strides=(1, 1),  # Stride: 2\n",
    "            padding='SAME',  # \"same\" padding\n",
    "            activation=None,  # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=101),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv1.1'\n",
    "        )\n",
    "\n",
    "        conv11 = tf.layers.batch_normalization(\n",
    "            conv11,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1.1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv11 = tf.nn.relu(conv11, name='relu1.1')\n",
    "\n",
    "\n",
    "    with tf.name_scope('conv1.2') as scope:\n",
    "        conv12 = tf.layers.conv2d(\n",
    "            conv11,  # Input data\n",
    "            filters=32,  # 32 filters\n",
    "            kernel_size=(3, 3),  # Kernel size: 5x5\n",
    "            strides=(1, 1),  # Stride: 2\n",
    "            padding='SAME',  # \"same\" padding\n",
    "            activation=None,  # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1101),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv1.2'\n",
    "        )\n",
    "\n",
    "        conv12 = tf.layers.batch_normalization(\n",
    "            conv12,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn1.2'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv12 = tf.nn.relu(conv12, name='relu1.1')\n",
    "\n",
    "    # Max pooling layer 1\n",
    "    with tf.name_scope('pool1') as scope:\n",
    "        pool1 = tf.layers.max_pooling2d(\n",
    "            conv12,  # Input\n",
    "            pool_size=(3, 3),  # Pool size: 3x3\n",
    "            strides=(2, 2),  # Stride: 2\n",
    "            padding='SAME',  # \"same\" padding\n",
    "            name='pool1'\n",
    "        )\n",
    "\n",
    "        # optional dropout\n",
    "        if dropout:\n",
    "            pool1 = tf.layers.dropout(pool1, rate=pooldropout_rate, seed=103, training=training)\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    with tf.name_scope('conv2.1') as scope:\n",
    "        conv2 = tf.layers.conv2d(\n",
    "            pool1,  # Input data\n",
    "            filters=64,  # 32 filters\n",
    "            kernel_size=(3, 3),  # Kernel size: 9x9\n",
    "            strides=(1, 1),  # Stride: 1\n",
    "            padding='SAME',  # \"same\" padding\n",
    "            activation=None,  # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=104),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv2.1'\n",
    "        )\n",
    "\n",
    "        conv2 = tf.layers.batch_normalization(\n",
    "            conv2,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2.1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv2 = tf.nn.relu(conv2, name='relu2.1')\n",
    "\n",
    "    # Convolutional layer 2\n",
    "    with tf.name_scope('conv2.2') as scope:\n",
    "        conv22 = tf.layers.conv2d(\n",
    "            conv2,  # Input data\n",
    "            filters=64,  # 32 filters\n",
    "            kernel_size=(3, 3),  # Kernel size: 9x9\n",
    "            strides=(1, 1),  # Stride: 1\n",
    "            padding='SAME',  # \"same\" padding\n",
    "            activation=None,  # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1104),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv2.2'\n",
    "        )\n",
    "\n",
    "        conv22 = tf.layers.batch_normalization(\n",
    "            conv22,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn2.2'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv22 = tf.nn.relu(conv22, name='relu2.2')\n",
    "\n",
    "    # Max pooling layer 2\n",
    "    with tf.name_scope('pool2') as scope:\n",
    "        pool2 = tf.layers.max_pooling2d(\n",
    "            conv22,  # Input\n",
    "            pool_size=(2, 2),  # Pool size: 3x3\n",
    "            strides=(2, 2),  # Stride: 2\n",
    "            padding='SAME',  # \"same\" padding\n",
    "            name='pool2'\n",
    "        )\n",
    "\n",
    "        # optional dropout\n",
    "        if dropout:\n",
    "            pool2 = tf.layers.dropout(pool2, rate=pooldropout_rate, seed=106, training=training)\n",
    "\n",
    "    # Convolutional layer 3\n",
    "    with tf.name_scope('conv3.1') as scope:\n",
    "        conv3 = tf.layers.conv2d(\n",
    "            pool2,  # Input data\n",
    "            filters=128,  # 48 filters\n",
    "            kernel_size=(3, 3),  # Kernel size: 5x5\n",
    "            strides=(1, 1),  # Stride: 1\n",
    "            padding='SAME',  # \"same\" padding\n",
    "            activation=None,  # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=107),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv3.1'\n",
    "        )\n",
    "\n",
    "        conv3 = tf.layers.batch_normalization(\n",
    "            conv3,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3.1'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv3 = tf.nn.relu(conv3, name='relu3.1')\n",
    "\n",
    "    # Convolutional layer 3\n",
    "    with tf.name_scope('conv3.2') as scope:\n",
    "        conv32 = tf.layers.conv2d(\n",
    "            conv3,  # Input data\n",
    "            filters=128,  # 48 filters\n",
    "            kernel_size=(3, 3),  # Kernel size: 5x5\n",
    "            strides=(1, 1),  # Stride: 1\n",
    "            padding='SAME',  # \"same\" padding\n",
    "            activation=None,  # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=1107),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv3.2'\n",
    "        )\n",
    "\n",
    "        conv32 = tf.layers.batch_normalization(\n",
    "            conv32,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn3.2'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv32 = tf.nn.relu(conv32, name='relu3.2')\n",
    "\n",
    "    # Max pooling layer 3\n",
    "    with tf.name_scope('pool3') as scope:\n",
    "        pool3 = tf.layers.max_pooling2d(\n",
    "            conv32,  # Input\n",
    "            pool_size=(2, 2),  # Pool size: 2x2\n",
    "            strides=(2, 2),  # Stride: 2\n",
    "            padding='SAME',  # \"same\" padding\n",
    "            name='pool3'\n",
    "        )\n",
    "\n",
    "        if dropout:\n",
    "            pool3 = tf.layers.dropout(pool3, rate=pooldropout_rate, seed=109, training=training)\n",
    "\n",
    "    # Convolutional layer 4\n",
    "    with tf.name_scope('conv4') as scope:\n",
    "            conv4 = tf.layers.conv2d(\n",
    "                pool3,  # Input data\n",
    "                filters=256,  # 48 filters\n",
    "                kernel_size=(3, 3),  # Kernel size: 5x5\n",
    "                strides=(1, 1),  # Stride: 1\n",
    "                padding='SAME',  # \"same\" padding\n",
    "                activation=None,  # None\n",
    "                kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=110),\n",
    "                kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "                name='conv4'\n",
    "            )\n",
    "\n",
    "            conv4 = tf.layers.batch_normalization(\n",
    "                conv4,\n",
    "                axis=-1,\n",
    "                momentum=0.99,\n",
    "                epsilon=epsilon,\n",
    "                center=True,\n",
    "                scale=True,\n",
    "                beta_initializer=tf.zeros_initializer(),\n",
    "                gamma_initializer=tf.ones_initializer(),\n",
    "                moving_mean_initializer=tf.zeros_initializer(),\n",
    "                moving_variance_initializer=tf.ones_initializer(),\n",
    "                training=training,\n",
    "                name='bn4'\n",
    "            )\n",
    "\n",
    "            # apply relu\n",
    "            conv4_bn_relu = tf.nn.relu(conv4, name='relu4')\n",
    "\n",
    "            #if dropout:\n",
    "            #    conv4_bn_relu = tf.layers.dropout(conv4_bn_relu, rate=convdropout_rate, seed=111, training=training)\n",
    "\n",
    "    # Max pooling layer 4\n",
    "    with tf.name_scope('pool4') as scope:\n",
    "            pool4 = tf.layers.max_pooling2d(\n",
    "                conv4_bn_relu,  # Input\n",
    "                pool_size=(2, 2),  # Pool size: 2x2\n",
    "                strides=(2, 2),  # Stride: 2\n",
    "                padding='SAME',  # \"same\" padding\n",
    "                name='pool4'\n",
    "            )\n",
    "\n",
    "            if dropout:\n",
    "                pool4 = tf.layers.dropout(pool4, rate=pooldropout_rate, seed=112, training=training)\n",
    "\n",
    "            # Convolutional layer 4\n",
    "    with tf.name_scope('conv5') as scope:\n",
    "        conv5 = tf.layers.conv2d(\n",
    "            pool4,  # Input data\n",
    "            filters=512,  # 48 filters\n",
    "            kernel_size=(3, 3),  # Kernel size: 5x5\n",
    "            strides=(1, 1),  # Stride: 1\n",
    "            padding='SAME',  # \"same\" padding\n",
    "            activation=None,  # None\n",
    "            kernel_initializer=tf.truncated_normal_initializer(stddev=5e-2, seed=113),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamC),\n",
    "            name='conv5'\n",
    "        )\n",
    "\n",
    "        conv5 = tf.layers.batch_normalization(\n",
    "            conv5,\n",
    "            axis=-1,\n",
    "            momentum=0.99,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn5'\n",
    "        )\n",
    "\n",
    "        # apply relu\n",
    "        conv5_bn_relu = tf.nn.relu(conv5, name='relu5')\n",
    "\n",
    "    # Max pooling layer 4\n",
    "    with tf.name_scope('pool5') as scope:\n",
    "        pool5 = tf.layers.max_pooling2d(\n",
    "            conv5_bn_relu,\n",
    "            pool_size=(2, 2),  # Pool size: 2x2\n",
    "            strides=(2, 2),  # Stride: 2\n",
    "            padding='SAME',\n",
    "            name='pool5'\n",
    "        )\n",
    "\n",
    "        if dropout:\n",
    "            pool5 = tf.layers.dropout(pool5, rate=pooldropout_rate, seed=115, training=training)\n",
    "\n",
    "    # Flatten output\n",
    "    with tf.name_scope('flatten') as scope:\n",
    "        flat_output = tf.contrib.layers.flatten(pool5)\n",
    "\n",
    "        # global average pooling?\n",
    "        # flat_output = tf.reduce_mean(pool5, axis=[1, 2])\n",
    "\n",
    "        # dropout at fc rate\n",
    "        flat_output = tf.layers.dropout(flat_output, rate=fcdropout_rate, seed=116, training=training)\n",
    "\n",
    "    # Fully connected layer 1\n",
    "    with tf.name_scope('fc1') as scope:\n",
    "        fc1 = tf.layers.dense(\n",
    "            flat_output,\n",
    "            2048,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=117),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamF),\n",
    "            name=\"fc1\"\n",
    "        )\n",
    "\n",
    "        bn_fc1 = tf.layers.batch_normalization(\n",
    "            fc1,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn_fc1'\n",
    "        )\n",
    "\n",
    "        fc1_relu = tf.nn.relu(bn_fc1, name='fc1_relu')\n",
    "\n",
    "        # dropout\n",
    "        fc1_relu = tf.layers.dropout(fc1_relu, rate=fcdropout_rate, seed=118, training=training)\n",
    "\n",
    "    # Fully connected layer 2\n",
    "    with tf.name_scope('fc2') as scope:\n",
    "        fc2 = tf.layers.dense(\n",
    "            fc1_relu,  # input\n",
    "            2048,  # 1024 hidden units\n",
    "            activation=None,  # None\n",
    "            kernel_initializer=tf.variance_scaling_initializer(scale=2, seed=119),\n",
    "            bias_initializer=tf.zeros_initializer(),\n",
    "            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=lamF),\n",
    "            name=\"fc2\"\n",
    "        )\n",
    "\n",
    "        bn_fc2 = tf.layers.batch_normalization(\n",
    "            fc2,\n",
    "            axis=-1,\n",
    "            momentum=0.9,\n",
    "            epsilon=epsilon,\n",
    "            center=True,\n",
    "            scale=True,\n",
    "            beta_initializer=tf.zeros_initializer(),\n",
    "            gamma_initializer=tf.ones_initializer(),\n",
    "            moving_mean_initializer=tf.zeros_initializer(),\n",
    "            moving_variance_initializer=tf.ones_initializer(),\n",
    "            training=training,\n",
    "            name='bn_fc2'\n",
    "        )\n",
    "\n",
    "        fc2_relu = tf.nn.relu(bn_fc2, name='fc2_relu')\n",
    "\n",
    "        # dropout\n",
    "        fc2_relu = tf.layers.dropout(fc2_relu, rate=fcdropout_rate, seed=120, training=training)\n",
    "\n",
    "    # Output layer\n",
    "    logits = tf.layers.dense(\n",
    "        fc2_relu,\n",
    "        num_classes,      # One output unit per category\n",
    "        activation=None,  # No activation function\n",
    "        kernel_initializer=tf.variance_scaling_initializer(scale=1, seed=121),\n",
    "        bias_initializer=tf.zeros_initializer(),\n",
    "        name=\"logits\"\n",
    "    )\n",
    "\n",
    "    # get the fully connected variables so we can only train them when retraining the network\n",
    "    fc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"fc\")\n",
    "\n",
    "    with tf.variable_scope('conv1', reuse=True):\n",
    "        conv_kernels1 = tf.get_variable('kernel')\n",
    "        kernel_transposed = tf.transpose(conv_kernels1, [3, 0, 1, 2])\n",
    "\n",
    "    with tf.variable_scope('visualization'):\n",
    "        tf.summary.image('conv1/filters', kernel_transposed, max_outputs=32, collections=[\"kernels\"])\n",
    "\n",
    "    ## Loss function options\n",
    "    # Regular mean cross entropy\n",
    "    #mean_ce = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "    # Different weighting method\n",
    "    # This will weight the positive examples higher so as to improve recall\n",
    "    weights = tf.multiply(weight, tf.cast(tf.greater(y, 0), tf.int32)) + 1\n",
    "    mean_ce = tf.reduce_mean(tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits, weights=weights))\n",
    "\n",
    "    # Add in l2 loss\n",
    "    loss = mean_ce + tf.losses.get_regularization_loss()\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    # Minimize cross-entropy - freeze certain layers depending on input\n",
    "    if freeze:\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step, var_list=fc_vars)\n",
    "    else:\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    # get the probabilites for the classes\n",
    "    probabilities = tf.nn.softmax(logits, name=\"probabilities\")\n",
    "    abnormal_probability = 1 - probabilities[:,0]\n",
    "\n",
    "    # Compute predictions from the probabilities\n",
    "    predictions = tf.argmax(probabilities, axis=1, output_type=tf.int64)\n",
    "\n",
    "    # get the accuracy\n",
    "    accuracy, acc_op = tf.metrics.accuracy(\n",
    "        labels=y,\n",
    "        predictions=predictions,\n",
    "        updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
    "        name=\"accuracy\",\n",
    "    )\n",
    "\n",
    "    # calculate recall\n",
    "    if num_classes > 2:\n",
    "        # collapse the predictions down to normal or not for our pr metrics\n",
    "        zero = tf.constant(0, dtype=tf.int64)\n",
    "        collapsed_predictions = tf.cast(tf.greater(abnormal_probability, threshold), tf.int32)\n",
    "        collapsed_labels = tf.greater(y, 0)\n",
    "\n",
    "        recall, rec_op = tf.metrics.recall(labels=collapsed_labels, predictions=collapsed_predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"recall\")\n",
    "        precision, prec_op = tf.metrics.precision(labels=collapsed_labels, predictions=collapsed_predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"precision\")\n",
    "\n",
    "    else:\n",
    "        recall, rec_op = tf.metrics.recall(labels=y, predictions=predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"recall\")\n",
    "        precision, prec_op = tf.metrics.precision(labels=y, predictions=predictions, updates_collections=tf.GraphKeys.UPDATE_OPS, name=\"precision\")\n",
    "\n",
    "    f1_score = 2 * ((precision * recall) / (precision + recall))\n",
    "    _, update_op = summary_lib.pr_curve_streaming_op(name='pr_curve',\n",
    "                                                     predictions=abnormal_probability,\n",
    "                                                     labels=y,\n",
    "                                                     updates_collections=tf.GraphKeys.UPDATE_OPS,\n",
    "                                                     num_thresholds=20)\n",
    "\n",
    "    tf.summary.scalar('recall_1', recall, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('precision_1', precision, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('f1_score', f1_score, collections=[\"summaries\"])\n",
    "\n",
    "    # Create summary hooks\n",
    "    tf.summary.scalar('accuracy', accuracy, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('cross_entropy', mean_ce, collections=[\"summaries\"])\n",
    "    tf.summary.scalar('learning_rate', learning_rate, collections=[\"summaries\"])\n",
    "\n",
    "    # add this so that the batch norm gets run\n",
    "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "    # Merge all the summaries\n",
    "    merged = tf.summary.merge_all(\"summaries\")\n",
    "    kernel_summaries = tf.summary.merge_all(\"kernels\")\n",
    "    per_epoch_summaries = [[]]\n",
    "\n",
    "    print(\"Graph created...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CONFIGURE OPTIONS\n",
    "# if we are initializing the weights from a pre-trained model make sure it exists\n",
    "if init_model is not None:\n",
    "    if os.path.exists(os.path.join(\"model\", init_model + '.ckpt.index')):\n",
    "        init = False\n",
    "    else:\n",
    "        init = True\n",
    "# otherwise if this model exists let's continue training it rather than reinitializing it\n",
    "else:\n",
    "    if os.path.exists(os.path.join(\"model\", model_name + '.ckpt.index')):\n",
    "        init = False\n",
    "    else:\n",
    "        init = True\n",
    "\n",
    "meta_data_every = 1\n",
    "log_to_tensorboard = True\n",
    "print_every = 5  # how often to print metrics\n",
    "checkpoint_every = 1  # how often to save model in epochs\n",
    "use_gpu = False  # whether or not to use the GPU\n",
    "print_metrics = True  # whether to print or plot metrics, if False a plot will be created and updated every epoch\n",
    "\n",
    "# Placeholders for metrics\n",
    "train_cost_values = []\n",
    "train_lr_values = []\n",
    "train_acc_values = []\n",
    "train_recall_values = []\n",
    "valid_acc_values = []\n",
    "valid_cost_values = []\n",
    "valid_recall_values = []\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allocator_type = 'BFC'\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a cut and paste of the code used to train the model. The code was copied from candidate_1.0.0.28.py. Attempting to run this on my laptop usually crashed it, so be aware before attempting to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## train the model\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    if log_to_tensorboard:\n",
    "        train_writer = tf.summary.FileWriter('./logs/tr_' + model_name, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter('./logs/te_' + model_name)\n",
    "\n",
    "    if not print_metrics:\n",
    "        # create a plot to be updated as model is trained\n",
    "        f, ax = plt.subplots(1, 4, figsize=(24, 5))\n",
    "\n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    # If the model is new initialize variables, else restore the session\n",
    "    if init:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Initializing model...\")\n",
    "    else:\n",
    "        # if we are initializing with the weights from another model load it\n",
    "        if init_model is not None:\n",
    "            # initialize the global variables\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # create the initializer function to initialize the weights, excluding the layers we want to retrain\n",
    "            init_fn = load_weights(init_model, exclude=[\"conv5\", \"bn5\", \"fc1\", \"bn_fc1\", \"bn_fc2\", \"fc3\", \"bn_fc3\", \"fc2\", \"fc_logits\", \"global_step\"])\n",
    "\n",
    "            # run the initializer\n",
    "            init_fn(sess)\n",
    "\n",
    "            # reset the global step\n",
    "            initial_global_step = global_step.assign(0)\n",
    "            sess.run(initial_global_step)\n",
    "\n",
    "            print(\"Initializing weights from model\", init_model)\n",
    "\n",
    "            # reset init model so we don't do this again\n",
    "            init_model = None\n",
    "        # otherwise load this model\n",
    "        else:\n",
    "            saver.restore(sess, './model/' + model_name + '.ckpt')\n",
    "            print(\"Restoring model\", model_name)\n",
    "\n",
    "    # if we are training the model\n",
    "    if action == \"train\":\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        print(\"Training model\", model_name, \"...\")\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            # Accuracy values (train) after each batch\n",
    "            batch_acc = []\n",
    "            batch_cost = []\n",
    "            batch_recall = []\n",
    "\n",
    "            for i in range(steps_per_epoch):\n",
    "                # create the metadata\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "\n",
    "                # Run training op and update ops\n",
    "                if (i % 50 != 0) or (i == 0):\n",
    "                    # log the kernel images once per epoch\n",
    "                    if (i == (steps_per_epoch - 1)) and log_to_tensorboard:\n",
    "                        _, _, _, image_summary, step = sess.run(\n",
    "                            [train_op, extra_update_ops, update_op, kernel_summaries, global_step],\n",
    "                            feed_dict={\n",
    "                                training: True,\n",
    "                            },\n",
    "                            options=run_options,\n",
    "                            run_metadata=run_metadata)\n",
    "\n",
    "                        # write the summary\n",
    "                        train_writer.add_summary(image_summary, step)\n",
    "                    else:\n",
    "                        _, _, _, step = sess.run(\n",
    "                            [train_op, extra_update_ops, update_op, global_step],\n",
    "                                feed_dict={\n",
    "                                    training: True,\n",
    "                                },\n",
    "                                options=run_options,\n",
    "                                run_metadata=run_metadata)\n",
    "\n",
    "                # every 50th step get the metrics\n",
    "                else:\n",
    "                    _, _, _, precision_value, summary, acc_value, cost_value, recall_value, step, lr = sess.run(\n",
    "                        [train_op, extra_update_ops, update_op, prec_op, merged, accuracy, mean_ce, rec_op, global_step, learning_rate],\n",
    "                        feed_dict={\n",
    "                            training: True,\n",
    "                        },\n",
    "                        options=run_options,\n",
    "                        run_metadata=run_metadata)\n",
    "\n",
    "                    # Save accuracy (current batch)\n",
    "                    batch_acc.append(acc_value)\n",
    "                    batch_cost.append(cost_value)\n",
    "                    batch_recall.append(recall_value)\n",
    "\n",
    "                    # log the summaries to tensorboard every 50 steps\n",
    "                    if log_to_tensorboard:\n",
    "                        # write the summary\n",
    "                        train_writer.add_summary(summary, step)\n",
    "\n",
    "                # only log the meta data once per epoch\n",
    "                if i == 1:\n",
    "                    train_writer.add_run_metadata(run_metadata, 'step %d' % step)\n",
    "\n",
    "            # save checkpoint every nth epoch\n",
    "            if (epoch % checkpoint_every == 0):\n",
    "                print(\"Saving checkpoint\")\n",
    "                save_path = saver.save(sess, './model/' + model_name + '.ckpt')\n",
    "\n",
    "                # Now that model is saved set init to false so we reload it next time\n",
    "                init = False\n",
    "\n",
    "            # init batch arrays\n",
    "            batch_cv_acc = []\n",
    "            batch_cv_loss = []\n",
    "            batch_cv_recall = []\n",
    "\n",
    "            # initialize the local variables so we have metrics only on the evaluation\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            print(\"Evaluating model...\")\n",
    "            # load the test data\n",
    "            X_cv, y_cv = load_validation_data(percentage=1, how=how, which=dataset)\n",
    "\n",
    "            # evaluate the test data\n",
    "            for X_batch, y_batch in get_batches(X_cv, y_cv, batch_size, distort=False):\n",
    "                _, _, valid_acc, valid_recall, valid_precision, valid_fscore, valid_cost = sess.run(\n",
    "                    [update_op, extra_update_ops, accuracy, rec_op, prec_op, f1_score, mean_ce],\n",
    "                    feed_dict={\n",
    "                        X: X_batch,\n",
    "                        y: y_batch,\n",
    "                        training: False\n",
    "                    })\n",
    "\n",
    "                batch_cv_acc.append(valid_acc)\n",
    "                batch_cv_loss.append(valid_cost)\n",
    "                batch_cv_recall.append(valid_recall)\n",
    "\n",
    "            # Write average of validation data to summary logs\n",
    "            if log_to_tensorboard:\n",
    "                # evaluate once more to get the summary, which will then be written to tensorboard\n",
    "                summary, cv_accuracy = sess.run(\n",
    "                    [merged, accuracy],\n",
    "                    feed_dict={\n",
    "                        X: X_cv[0:2],\n",
    "                        y: y_cv[0:2],\n",
    "                        training: False\n",
    "                    })\n",
    "\n",
    "            test_writer.add_summary(summary, step)\n",
    "            # test_writer.add_summary(other_summaries, step)\n",
    "            step += 1\n",
    "\n",
    "            # delete the test data to save memory\n",
    "            del (X_cv)\n",
    "            del (y_cv)\n",
    "\n",
    "            print(\"Done evaluating...\")\n",
    "\n",
    "            # take the mean of the values to add to the metrics\n",
    "            valid_acc_values.append(np.mean(batch_cv_acc))\n",
    "            train_acc_values.append(np.mean(batch_acc))\n",
    "\n",
    "            valid_cost_values.append(np.mean(batch_cv_loss))\n",
    "            train_cost_values.append(np.mean(batch_cost))\n",
    "\n",
    "            valid_recall_values.append(np.mean(batch_cv_recall))\n",
    "            train_recall_values.append(np.mean(batch_recall))\n",
    "\n",
    "            train_lr_values.append(lr)\n",
    "\n",
    "            # save the metrics\n",
    "            np.save(os.path.join(\"data\", model_name + \"train_acc.npy\"), train_acc_values)\n",
    "            np.save(os.path.join(\"data\", model_name + \"cv_acc.npy\"), valid_acc_values)\n",
    "\n",
    "            np.save(os.path.join(\"data\", model_name + \"train_loss.npy\"), train_cost_values)\n",
    "            np.save(os.path.join(\"data\", model_name + \"cv_loss.npy\"), valid_cost_values)\n",
    "\n",
    "            np.save(os.path.join(\"data\", model_name + \"train_recall.npy\"), train_recall_values)\n",
    "            np.save(os.path.join(\"data\", model_name + \"cv_recall.npy\"), valid_recall_values)\n",
    "\n",
    "            np.save(os.path.join(\"data\", model_name + \"train_lr.npy\"), train_lr_values)\n",
    "\n",
    "            # Print progress every nth epoch to keep output to reasonable amount\n",
    "            if (epoch % print_every == 0):\n",
    "                print(\n",
    "                'Epoch {:02d} - step {} - cv acc: {:.4f} - train acc: {:.3f} (mean)'.format(\n",
    "                    epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc)\n",
    "                ))\n",
    "\n",
    "            # Print data every 50th epoch so I can write it down to compare models\n",
    "            if (not print_metrics) and (epoch % 50 == 0) and (epoch > 1):\n",
    "                if (epoch % print_every == 0):\n",
    "                    print(\n",
    "                    'Epoch {:02d} - step {} - cv acc: {:.4f} - train acc: {:.3f} (mean)'.format(\n",
    "                        epoch, step, np.mean(batch_cv_acc), np.mean(batch_acc)\n",
    "                    ))\n",
    "\n",
    "        # stop the coordinator\n",
    "        coord.request_stop()\n",
    "\n",
    "        # Wait for threads to stop\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results\n",
    "\n",
    "The metrics created in training were saved to disk and are plotted in the notebook convnet_training_metrics.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Data\n",
    "\n",
    "Note that in order to evaluate the model for binary classification the graph needs to be recreated with num_labels = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.0.0.35b.98 trained for binary classification on Dataset 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model_s1.0.0.35b.98.ckpt\n",
      "Restoring model model_s1.0.0.35b.98\n",
      "Evaluating on test data\n",
      "Mean Test Accuracy: 0.9554958\n",
      "Mean Test Recall: 0.8491812\n"
     ]
    }
   ],
   "source": [
    "how = \"normal\"\n",
    "dataset = 9\n",
    "model_name = \"model_s1.0.0.35b.98\"\n",
    "\n",
    "## Evaluate on test data multi-class\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # restore the model\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    saver.restore(sess, './model/' + model_name + '.ckpt')\n",
    "    print(\"Restoring model\", model_name)\n",
    "    \n",
    "    # load the test data\n",
    "    X_te, y_te = load_validation_data(how=how, data=\"test\", which=dataset)\n",
    "    print(\"Evaluating on test data\")\n",
    "    \n",
    "    test_accuracy = []\n",
    "    test_recall = []\n",
    "    test_predictions = []\n",
    "    ground_truth = []\n",
    "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, distort=False):\n",
    "        _, yhat, test_acc_value, test_recall_value = sess.run([extra_update_ops, predictions, accuracy, rec_op], feed_dict=\n",
    "        {\n",
    "            X: X_batch,\n",
    "            y: y_batch,\n",
    "            training: False\n",
    "        })\n",
    "\n",
    "        test_accuracy.append(test_acc_value)\n",
    "        test_recall.append(test_recall_value)\n",
    "        test_predictions.append(yhat)\n",
    "        ground_truth.append(y_batch)\n",
    "\n",
    "    # print the results\n",
    "    print(\"Mean Test Accuracy:\", np.mean(test_accuracy))\n",
    "    print(\"Mean Test Recall:\", np.mean(test_recall))\n",
    "    \n",
    "    # unlist the predictions and truth\n",
    "    test_predictions = flatten(test_predictions)\n",
    "    ground_truth = flatten(ground_truth)\n",
    "    \n",
    "    # save the predictions and truth for review\n",
    "    np.save(os.path.join(\"data\", \"predictions_\" + model_name + \".npy\"), test_predictions)\n",
    "    np.save(os.path.join(\"data\", \"truth_\" + model_name + \".npy\"), ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate on MIAS Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model_s1.0.0.35b.98.ckpt\n",
      "Restoring model model_s1.0.0.35b.98\n",
      "Evaluating on MIAS test data\n",
      "Mean Test Accuracy: 0.88686436\n",
      "Mean Test Recall: 0.7967232\n"
     ]
    }
   ],
   "source": [
    "how = \"normal\"\n",
    "dataset = 9\n",
    "model_name = \"model_s1.0.0.35b.98\"\n",
    "\n",
    "## Evaluate on test data multi-class\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # restore the model\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    saver.restore(sess, './model/' + model_name + '.ckpt')\n",
    "    print(\"Restoring model\", model_name)\n",
    "    \n",
    "    # load the test data\n",
    "    X_te, y_te = load_validation_data(how=how, data=\"mias\", which=dataset)\n",
    "    print(\"Evaluating on MIAS test data\")\n",
    "    \n",
    "    test_accuracy = []\n",
    "    test_recall = []\n",
    "    test_predictions = []\n",
    "    ground_truth = []\n",
    "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, distort=False):\n",
    "        _, yhat, test_acc_value, test_recall_value = sess.run([extra_update_ops, predictions, accuracy, rec_op], feed_dict=\n",
    "        {\n",
    "            X: X_batch,\n",
    "            y: y_batch,\n",
    "            training: False\n",
    "        })\n",
    "\n",
    "        test_accuracy.append(test_acc_value)\n",
    "        test_recall.append(test_recall_value)\n",
    "        test_predictions.append(yhat)\n",
    "        ground_truth.append(y_batch)\n",
    "\n",
    "    # print the results\n",
    "    print(\"Mean Test Accuracy:\", np.mean(test_accuracy))\n",
    "    print(\"Mean Test Recall:\", np.mean(test_recall))\n",
    "    \n",
    "    # unlist the predictions and truth\n",
    "    test_predictions = flatten(test_predictions)\n",
    "    ground_truth = flatten(ground_truth)\n",
    "    \n",
    "    # save the predictions and truth for review\n",
    "    np.save(os.path.join(\"data\", \"predictions_\" + model_name + \".npy\"), test_predictions)\n",
    "    np.save(os.path.join(\"data\", \"truth_\" + model_name + \".npy\"), ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1.0.0.35b.96 trained for binary classification on Dataset 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model_s1.0.0.35b.96.bu30.ckpt\n",
      "Restoring model model_s1.0.0.35b.96.bu30\n",
      "Evaluating on test data\n",
      "Mean Test Accuracy: 0.93031216\n",
      "Mean Test Recall: 0.853614\n"
     ]
    }
   ],
   "source": [
    "how = \"normal\"\n",
    "dataset = 9\n",
    "model_name = \"model_s1.0.0.35b.96.bu30\"\n",
    "\n",
    "## Evaluate on test data multi-class\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # restore the model\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    saver.restore(sess, './model/' + model_name + '.ckpt')\n",
    "    print(\"Restoring model\", model_name)\n",
    "    \n",
    "    # load the test data\n",
    "    X_te, y_te = load_validation_data(how=how, data=\"test\", which=dataset)\n",
    "    print(\"Evaluating on test data\")\n",
    "    \n",
    "    test_accuracy = []\n",
    "    test_recall = []\n",
    "    test_predictions = []\n",
    "    ground_truth = []\n",
    "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, distort=False):\n",
    "        _, yhat, test_acc_value, test_recall_value = sess.run([extra_update_ops, predictions, accuracy, rec_op], feed_dict=\n",
    "        {\n",
    "            X: X_batch,\n",
    "            y: y_batch,\n",
    "            training: False\n",
    "        })\n",
    "\n",
    "        test_accuracy.append(test_acc_value)\n",
    "        test_recall.append(test_recall_value)\n",
    "        test_predictions.append(yhat)\n",
    "        ground_truth.append(y_batch)\n",
    "\n",
    "    # print the results\n",
    "    print(\"Mean Test Accuracy:\", np.mean(test_accuracy))\n",
    "    print(\"Mean Test Recall:\", np.mean(test_recall))\n",
    "    \n",
    "    # unlist the predictions and truth\n",
    "    test_predictions = flatten(test_predictions)\n",
    "    ground_truth = flatten(ground_truth)\n",
    "    \n",
    "    # save the predictions and truth for review\n",
    "    np.save(os.path.join(\"data\", \"predictions_\" + model_name + \".npy\"), test_predictions)\n",
    "    np.save(os.path.join(\"data\", \"truth_\" + model_name + \".npy\"), ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And evaluate on the MIAS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model_s1.0.0.35b.96.bu30.ckpt\n",
      "Restoring model model_s1.0.0.35b.96.bu30\n",
      "Evaluating on MIAS test data\n",
      "Mean Test Accuracy: 0.8129536\n",
      "Mean Test Recall: 0.8739727\n"
     ]
    }
   ],
   "source": [
    "how = \"normal\"\n",
    "dataset = 9\n",
    "model_name = \"model_s1.0.0.35b.96.bu30\"\n",
    "\n",
    "## Evaluate on test data multi-class\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # restore the model\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    saver.restore(sess, './model/' + model_name + '.ckpt')\n",
    "    print(\"Restoring model\", model_name)\n",
    "    \n",
    "    # load the test data\n",
    "    X_te, y_te = load_validation_data(how=how, data=\"mias\", which=dataset)\n",
    "    print(\"Evaluating on MIAS test data\")\n",
    "    \n",
    "    test_accuracy = []\n",
    "    test_recall = []\n",
    "    test_predictions = []\n",
    "    ground_truth = []\n",
    "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, distort=False):\n",
    "        _, yhat, test_acc_value, test_recall_value = sess.run([extra_update_ops, predictions, accuracy, rec_op], feed_dict=\n",
    "        {\n",
    "            X: X_batch,\n",
    "            y: y_batch,\n",
    "            training: False\n",
    "        })\n",
    "\n",
    "        test_accuracy.append(test_acc_value)\n",
    "        test_recall.append(test_recall_value)\n",
    "        test_predictions.append(yhat)\n",
    "        ground_truth.append(y_batch)\n",
    "\n",
    "    # print the results\n",
    "    print(\"Mean Test Accuracy:\", np.mean(test_accuracy))\n",
    "    print(\"Mean Test Recall:\", np.mean(test_recall))\n",
    "    \n",
    "    # unlist the predictions and truth\n",
    "    test_predictions = flatten(test_predictions)\n",
    "    ground_truth = flatten(ground_truth)\n",
    "    \n",
    "    # save the predictions and truth for review\n",
    "    np.save(os.path.join(\"data\", \"predictions_\" + model_name + \".npy\"), test_predictions)\n",
    "    np.save(os.path.join(\"data\", \"truth_\" + model_name + \".npy\"), ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "The models below used a different graph than the models in the 1.0.0.35 family. Specifically they did not scale the input data and did not use data augmentation. If the code below is re-run the results will not match those below unless the graph is adjusted.\n",
    "\n",
    "These results are just included for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Trained on Dataset 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model_s1.0.0.29l.14.ckpt\n",
      "Restoring model model_s1.0.0.29l.14\n",
      "Evaluating on test data\n",
      "Mean Test Accuracy: 0.9050548\n",
      "Mean Test Recall: 0.9450482\n"
     ]
    }
   ],
   "source": [
    "how = \"label\"\n",
    "which = 6\n",
    "model_name = \"model_s1.0.0.29l.14\"\n",
    "\n",
    "## Evaluate on test data multi-class\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # restore the model\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    saver.restore(sess, './model/' + model_name + '.ckpt')\n",
    "    print(\"Restoring model\", model_name)\n",
    "    \n",
    "    # load the test data\n",
    "    X_te, y_te = load_validation_data(how=how, data=\"test\", which=dataset)\n",
    "    print(\"Evaluating on test data\")\n",
    "    \n",
    "    test_accuracy = []\n",
    "    test_recall = []\n",
    "    test_predictions = []\n",
    "    ground_truth = []\n",
    "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, distort=False):\n",
    "        _, yhat, test_acc_value, test_recall_value = sess.run([extra_update_ops, predictions, accuracy, rec_op], feed_dict=\n",
    "        {\n",
    "            X: X_batch,\n",
    "            y: y_batch,\n",
    "            training: False\n",
    "        })\n",
    "\n",
    "        test_accuracy.append(test_acc_value)\n",
    "        test_recall.append(test_recall_value)\n",
    "        test_predictions.append(yhat)\n",
    "        ground_truth.append(y_batch)\n",
    "\n",
    "    # print the results\n",
    "    print(\"Mean Test Accuracy:\", np.mean(test_accuracy))\n",
    "    print(\"Mean Test Recall:\", np.mean(test_recall))\n",
    "    \n",
    "    # unlist the predictions and truth\n",
    "    test_predictions = flatten(test_predictions)\n",
    "    ground_truth = flatten(ground_truth)\n",
    "    \n",
    "    # save the predictions and truth for review\n",
    "    np.save(os.path.join(\"data\", \"predictions_\" + model_name + \".npy\"), test_predictions)\n",
    "    np.save(os.path.join(\"data\", \"truth_\" + model_name + \".npy\"), ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model_s1.0.0.29l.14.ckpt\n",
      "Restoring model model_s1.0.0.29l.14\n",
      "Mean MIAS Accuracy: 0.14627369\n",
      "Mean MIAS Recall: 0.9783151\n"
     ]
    }
   ],
   "source": [
    "how = \"label\"\n",
    "which = 9\n",
    "## Evaluate on MIAS data\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # restore the model\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    saver.restore(sess, './model/' + model_name + '.ckpt')\n",
    "    print(\"Restoring model\", model_name)\n",
    "    \n",
    "    ## load the MIAS data\n",
    "    X_te, y_te = load_validation_data(how=how, data=\"mias\", which=dataset)\n",
    "\n",
    "    mias_test_accuracy = []\n",
    "    mias_test_recall = []\n",
    "    mias_test_predictions = []\n",
    "    mias_ground_truth = []\n",
    "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, distort=False):\n",
    "        _, yhat, test_acc_value, test_recall_value = sess.run([extra_update_ops, predictions, accuracy, rec_op], feed_dict=\n",
    "        {\n",
    "            X: X_batch,\n",
    "            y: y_batch,\n",
    "            training: False\n",
    "        })\n",
    "\n",
    "        mias_test_accuracy.append(test_acc_value)\n",
    "        mias_test_recall.append(test_recall_value)\n",
    "        mias_test_predictions.append(yhat)\n",
    "        mias_ground_truth.append(y_batch)\n",
    "\n",
    "    # print the results\n",
    "    print(\"Mean MIAS Accuracy:\", np.mean(mias_test_accuracy))\n",
    "    print(\"Mean MIAS Recall:\", np.mean(mias_test_recall))\n",
    "\n",
    "    # unlist the predictions and truth\n",
    "    mias_test_predictions = flatten(mias_test_predictions)\n",
    "    mias_ground_truth = flatten(mias_ground_truth)\n",
    "\n",
    "    # save the predictions and truth for review\n",
    "    np.save(os.path.join(\"data\", \"mias_predictions_\" + model_name + \".npy\"), mias_test_predictions)\n",
    "    np.save(os.path.join(\"data\", \"mias_truth_\" + model_name + \".npy\"), mias_ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Trained on Dataset 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model_s1.0.0.29l.8.2.ckpt\n",
      "Restoring model model_s1.0.0.29l.8.2\n",
      "Evaluating on test data\n",
      "\n",
      "Mean Test Accuracy: 0.99324185\n",
      "Mean Test Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "how = \"normal\"\n",
    "which = 8\n",
    "model_name = \"model_s1.0.0.29l.8.2\"\n",
    "## Evaluate on test data multi-class\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # restore the model\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    saver.restore(sess, './model/' + model_name + '.ckpt')\n",
    "    print(\"Restoring model\", model_name)\n",
    "    \n",
    "    # load the test data\n",
    "    X_te, y_te = load_validation_data(how=how, data=\"test\", which=dataset)\n",
    "    print(\"Evaluating on test data\")\n",
    "    \n",
    "    test_accuracy = []\n",
    "    test_recall = []\n",
    "    test_predictions = []\n",
    "    ground_truth = []\n",
    "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, distort=False):\n",
    "        _, yhat, test_acc_value, test_recall_value = sess.run([extra_update_ops, predictions, accuracy, rec_op], feed_dict=\n",
    "        {\n",
    "            X: X_batch,\n",
    "            y: y_batch,\n",
    "            training: False\n",
    "        })\n",
    "\n",
    "        test_accuracy.append(test_acc_value)\n",
    "        test_recall.append(test_recall_value)\n",
    "        test_predictions.append(yhat)\n",
    "        ground_truth.append(y_batch)\n",
    "\n",
    "    # print the results\n",
    "    print()\n",
    "    print(\"Mean Test Accuracy:\", np.mean(test_accuracy))\n",
    "    print(\"Mean Test Recall:\", np.mean(test_recall))\n",
    "    \n",
    "    # unlist the predictions and truth\n",
    "    test_predictions = flatten(test_predictions)\n",
    "    ground_truth = flatten(ground_truth)\n",
    "    \n",
    "    # save the predictions and truth for review\n",
    "    np.save(os.path.join(\"data\", \"predictions_\" + model_name + \".npy\"), test_predictions)\n",
    "    np.save(os.path.join(\"data\", \"truth_\" + model_name + \".npy\"), ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model_s1.0.0.29l.8.2.ckpt\n",
      "Restoring model model_s1.0.0.29l.8.2\n",
      "Evaluating on test data\n",
      "\n",
      "Mean Test Accuracy: 0.9944317\n",
      "Mean Test Recall: 0.9948756\n"
     ]
    }
   ],
   "source": [
    "# evaluate this model on test datset 6\n",
    "how = \"normal\"\n",
    "which = dataset = 6\n",
    "model_name = \"model_s1.0.0.29l.8.2\"\n",
    "## Evaluate on test data multi-class\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # restore the model\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    saver.restore(sess, './model/' + model_name + '.ckpt')\n",
    "    print(\"Restoring model\", model_name)\n",
    "    \n",
    "    # load the test data\n",
    "    X_te, y_te = load_validation_data(how=how, data=\"test\", which=dataset)\n",
    "    print(\"Evaluating on test data\")\n",
    "    \n",
    "    test_accuracy = []\n",
    "    test_recall = []\n",
    "    test_predictions = []\n",
    "    ground_truth = []\n",
    "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, distort=False):\n",
    "        _, yhat, test_acc_value, test_recall_value = sess.run([extra_update_ops, predictions, accuracy, rec_op], feed_dict=\n",
    "        {\n",
    "            X: X_batch,\n",
    "            y: y_batch,\n",
    "            training: False\n",
    "        })\n",
    "\n",
    "        test_accuracy.append(test_acc_value)\n",
    "        test_recall.append(test_recall_value)\n",
    "        test_predictions.append(yhat)\n",
    "        ground_truth.append(y_batch)\n",
    "\n",
    "    # print the results\n",
    "    print()\n",
    "    print(\"Mean Test Accuracy:\", np.mean(test_accuracy))\n",
    "    print(\"Mean Test Recall:\", np.mean(test_recall))\n",
    "    \n",
    "    # unlist the predictions and truth\n",
    "    test_predictions = flatten(test_predictions)\n",
    "    ground_truth = flatten(ground_truth)\n",
    "    \n",
    "    # save the predictions and truth for review\n",
    "    np.save(os.path.join(\"data\", \"predictions_\" + model_name + \".npy\"), test_predictions)\n",
    "    np.save(os.path.join(\"data\", \"truth_\" + model_name + \".npy\"), ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./model/model_s1.0.0.29l.8.2.ckpt\n",
      "Restoring model model_s1.0.0.29l.8.2\n",
      "\n",
      "MIAS Accuracy: 0.21897121\n",
      "MIAS Recall: 0.9831933\n",
      "MIAS Precision: 0.12413793\n"
     ]
    }
   ],
   "source": [
    "how = \"normal\"\n",
    "which = 9\n",
    "model_name = \"model_s1.0.0.29l.8.2\"\n",
    "## Evaluate on MIAS data\n",
    "with tf.Session(graph=graph, config=config) as sess:\n",
    "    # create the saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # restore the model\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    saver.restore(sess, './model/' + model_name + '.ckpt')\n",
    "    print(\"Restoring model\", model_name)\n",
    "    \n",
    "    ## load the MIAS data\n",
    "    X_te, y_te = load_validation_data(how=how, data=\"mias\", which=dataset)\n",
    "\n",
    "    mias_test_predictions = []\n",
    "    mias_ground_truth = []\n",
    "    mias_test_accuracy = []\n",
    "    mias_test_recall = []\n",
    "    \n",
    "    for X_batch, y_batch in get_batches(X_te, y_te, batch_size, distort=False):\n",
    "        _, yhat, test_acc_value, test_recall_value, test_precision_value = sess.run([extra_update_ops, predictions, accuracy, rec_op, prec_op], feed_dict=\n",
    "        {\n",
    "            X: X_batch,\n",
    "            y: y_batch,\n",
    "            training: False\n",
    "        })\n",
    "\n",
    "        mias_test_accuracy.append(test_acc_value)\n",
    "        mias_test_recall.append(test_recall_value)\n",
    "        mias_test_predictions.append(yhat)\n",
    "        mias_ground_truth.append(y_batch)\n",
    "    \n",
    "    # get the metrics - I think this may be more accuracte than adding them up myself\n",
    "    test_acc_value, test_recall_value, test_precision_value = sess.run([accuracy, recall, precision], feed_dict=\n",
    "        {\n",
    "            X: X_batch[0:1],\n",
    "            y: y_batch[0:1],\n",
    "            training: False\n",
    "        })\n",
    "    \n",
    "    print()\n",
    "    print(\"MIAS Accuracy:\", test_acc_value)\n",
    "    print(\"MIAS Recall:\", test_recall_value)\n",
    "    print(\"MIAS Precision:\", test_precision_value)\n",
    "    \n",
    "    # unlist the predictions and truth\n",
    "    mias_test_predictions = flatten(mias_test_predictions)\n",
    "    mias_ground_truth = flatten(mias_ground_truth)\n",
    "\n",
    "    # save the predictions and truth for review\n",
    "    np.save(os.path.join(\"data\", \"mias_predictions_\" + model_name + \".npy\"), mias_test_predictions)\n",
    "    np.save(os.path.join(\"data\", \"mias_truth_\" + model_name + \".npy\"), mias_ground_truth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
